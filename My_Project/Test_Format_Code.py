# -*- coding: utf-8 -*-
"""Copy of Test_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p1q8qfr_Nkl4OnoEYrfbcrcXtYskcWXR

<center>
<b><i><font color="Sky Blue" size="8">Test Cricket Analysis</font></i></b>
</center>

<h2><b><font color="gold">Introduction</font></b></h2>
"""

# Mounting Google Drive

from google.colab import drive
drive.mount('/content/drive')

# Importing required libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Importing warnings to ignore warnings

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Path for files from drive
test_ball_file = '/content/drive/MyDrive/Cricket_Project/data/model_ready/Ball By Ball/test_ball_by_ball.csv'
test_info_file = '/content/drive/MyDrive/Cricket_Project/data/model_ready/Info/test_info.csv'
test_summary_file = '/content/drive/MyDrive/Cricket_Project/data/model_ready/Summary/test_summary.csv'
test_venue_file = '/content/drive/MyDrive/Cricket_Project/data/model_ready/Test_match_id_venue_clean.csv'
players_file = '/content/drive/MyDrive/Cricket_Project/data/player_lookup_all_formats.csv'

# Loading the CSV's into a DataFrame
df_test_ball = pd.read_csv(test_ball_file, low_memory=False)
df_test_info = pd.read_csv(test_info_file, low_memory=False)
df_test_summary = pd.read_csv(test_summary_file, low_memory=False)
df_test_venue = pd.read_csv(test_venue_file, low_memory=False)
df_players_lookup = pd.read_csv(players_file, low_memory=False)

# Verifying the shape of Dataframes

print(f"Ball By Ball shape: {df_test_ball.shape}")
print(f"Info shape: {df_test_info.shape}")
print(f"Summary shape: {df_test_summary.shape}")
print(f"Venue shape: {df_test_venue.shape}")
print(f"Players shape: {df_players_lookup.shape}")

"""<h2><b><font color="gold">Data Cleaning</font></b></h2>"""

# Checking for missing values in each column
missing_counts = df_test_ball.isnull().sum()
print(missing_counts)

# Renaming columns for better readability

df_test_ball = df_test_ball.rename(columns={
    'runs.batter': 'batsman_runs',
    'runs.extras': 'extras',
    'runs.total': 'total_runs'
})

# This block cleans up the DataFrame by removing empty columns and filling missing values in key columns,
# such as runs and bowler, with sensible defaults. It also creates an 'is_wicket' column to mark if a wicket has fallen.

df_test_ball.dropna(axis=1, how='all', inplace=True)

if 'batsman_runs' in df_test_ball.columns:
    df_test_ball['batsman_runs'] = df_test_ball['batsman_runs'].fillna(0)

if 'extras' in df_test_ball.columns:
    df_test_ball['extras'] = df_test_ball['extras'].fillna(0)

if 'bowler' in df_test_ball.columns:
    df_test_ball['bowler'] = df_test_ball['bowler'].fillna("Unknown")

if 'wicket.kind' in df_test_ball.columns:
    df_test_ball['is_wicket'] = df_test_ball['wicket.kind'].notnull().astype(int)
else:
    df_test_ball['is_wicket'] = 0

"""<h2><b><font color="gold">Data Pre-Processing</font></b></h2>"""

# Standardizing the Match ID Column Name

for df in [df_test_ball, df_test_info, df_test_summary, df_test_venue]:
    if 'Match Id' in df.columns:
        df.rename(columns={'Match Id': 'Match_ID'}, inplace=True)

# List of columns we actually need from the info DataFrame.
required_info_cols = [
    'Match_ID', 'balls_per_over', 'city', 'dates', 'event.name',
    'event.match_number', 'gender', 'match_type', 'match_type_number',
    'season', 'teams', 'player_of_match', 'outcome.winner', 'outcome.by.runs',
    'outcome.by.wickets', 'outcome.result', 'outcome.summary'
]

# Keep only the columns from required_info_cols that are present in the DataFrame.
df_test_info = df_test_info[[col for col in required_info_cols if col in df_test_info.columns]].copy()

# Merging Ball data and Info data

df_test_ball = df_test_ball.merge(df_test_info, on='Match_ID', how='left')

# merging Ball data and venue data

df_test_ball = df_test_ball.merge(df_test_venue, on='Match_ID', how='left')

# Keeping only Test players from the player lookup table and remove any duplicate player names.
test_players_lookup = df_players_lookup[df_players_lookup['format'] == 'Test'].drop_duplicates(subset=['name'])

# Merge player_id into the main ball-by-ball DataFrame.
# For each ball, finding the player's ID by matching the batter's name with the Test player names.
df_test_ball = df_test_ball.merge(
    test_players_lookup[['player_id', 'name']],
    left_on='batter',
    right_on='name',
    how='left'
)

# Remove the extra 'name' column from the DataFrame after merging, since we only need the 'player_id', not the name itself.
df_test_ball = df_test_ball.drop(columns=['name'])

"""<h2><b><font color="gold">Finding batting team and opponent team for better analysis</font></b></h2>"""

# Finding opponent team
''' This code defines a function to identify the opposing team for each ball,
handling cases where the team names may be stored as a list or as a comma-separated string.
It then applies this function to the data, creating a new column with the opponent teamâ€™s name.'''


def get_opposition(row):
    teams = row['teams']
    if isinstance(teams, list):
        pass
    elif isinstance(teams, str):
        teams = [t.strip() for t in teams.split(',')]
    else:
        return None

    if row['batting_team'] == teams[0]:
        return teams[1]
    elif row['batting_team'] == teams[1]:
        return teams[0]
    else:
        return None

df_test_ball['opposition'] = df_test_ball.apply(get_opposition, axis=1)
print("Missing opposition count:", df_test_ball['opposition'].isna().sum())

# Finding Batting Team
if 'team' in df_test_ball.columns:
    df_test_ball['batting_team'] = df_test_ball['team']
print("Missing batting_team count:", df_test_ball['batting_team'].isna().sum())

"""<h2><b><font color="gold">Finding ball number of that over</font></b></h2>"""

# Finding ball number of that over
''' This function assigns ball numbers in the format 'over.ball' for each delivery,
 ensuring only legal balls (not wides or no-balls) increment the count within each over.
 It then adds these ball numbers as a new column in the DataFrame.'''


def assign_ball_numbers_exact(df):
    ball_numbers = []
    prev_over = None
    ball_count = 1

    for idx, row in df.iterrows():
        current_over = row['over']
        wide = row['extras.wides']
        noball = row['extras.noballs']

        # New over: reset ball count to 1
        if idx == 0 or current_over != prev_over:
            ball_count = 1

        ball_numbers.append(f"{current_over}.{ball_count}")

        # Only increment ball count if this ball is LEGAL
        if (wide == 0) and (noball == 0):
            if ball_count < 6:
                ball_count += 1
        # If not legal, ball_count does NOT increment

        prev_over = current_over

    df['ball_number'] = ball_numbers
    return df


df_test_ball = assign_ball_numbers_exact(df_test_ball)

"""<h2><b><font color="gold">Sorting Data Column</font></b></h2>"""

import pandas as pd

# 1) Convert 'dates' from list-like string to datetime
df_test_ball['dates'] = (
    df_test_ball['dates'].astype(str).str.extract(r'(\d{4}-\d{2}-\d{2})')[0]
)
df_test_ball['dates'] = pd.to_datetime(df_test_ball['dates'], errors='coerce')

# 2) Convert 'ball_number' to float
df_test_ball['ball_number'] = pd.to_numeric(df_test_ball['ball_number'], errors='coerce')

# Quick check
print(df_test_ball[['dates', 'ball_number']].dtypes)
print(df_test_ball[['dates', 'ball_number']].head())

"""<h2><b><font color="gold">Overall analysis visualizations</font></b></h2>"""

# Most Runs scored Battters in test

plt.figure(figsize=(12,5))
top_batsmen = df_test_ball.groupby('batter')['batsman_runs'].sum().sort_values(ascending=False).head(10)
sns.barplot(
    x=top_batsmen.values,
    y=top_batsmen.index,
    hue=top_batsmen.index,
    palette="viridis",
    legend=False
)

plt.title("Top 10 Most Test Run Scorers ", color = 'red')
plt.xlabel("Total Runs",  color='blue', fontsize=12)
plt.ylabel("Batter", color='green', fontsize=12)
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/Top_10_Test_Run_Scorers.png")
plt.show()

# Most wicket taking bowlers in test

plt.figure(figsize=(12,5))
top_bowlers = df_test_ball[df_test_ball['is_wicket']==1].groupby('bowler').size().sort_values(ascending=False).head(10)
sns.barplot(
    x=top_bowlers.values,
    y=top_bowlers.index,
    hue=top_bowlers.index,
    palette="viridis",
    legend=False
)
plt.xlabel('Wickets', color='blue', fontsize=12)
plt.ylabel('Bowler', color='green', fontsize=12)
plt.title('Top 10 Test Bowlers by Wickets', color = 'red')
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/Top_10_Test_Wicket_Takers.png")
plt.show()

# Total runs scored team for match in Tests

runs_per_innings = df_test_ball.groupby(['Match_ID', 'batting_team'])['total_runs'].sum().reset_index()

plt.figure(figsize=(14,6))
sns.boxplot(x='batting_team', y='total_runs', data=runs_per_innings)
plt.title("Distribution of Test Innings Totals by Batting Team", color = 'red')
plt.xlabel("Batting Team",  color='blue', fontsize=12)
plt.ylabel("Runs", color='green', fontsize=12)
plt.xticks(rotation=90)
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/Overall_Team_Test_Run_Scores.png")
plt.show()

# Total Wickets taken by Bowling team in Match
wickets_per_innings = df_test_ball.groupby(['Match_ID', 'opposition'])['is_wicket'].sum().reset_index()

plt.figure(figsize=(14,6))
sns.boxplot(x='opposition', y='is_wicket', data=wickets_per_innings)
plt.title("Distribution of Test Match Wickets by Bowling Team", color = 'red')
plt.xlabel("Bowling Team", color='blue', fontsize=12)
plt.ylabel("Wickets", color='green', fontsize=12)
plt.xticks(rotation=90)
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/Test_Overall_Wickets_Team_.png")
plt.show()

# Number of matches won by each team

df_test_info['outcome.winner'] = df_test_info['outcome.winner'].fillna('No Result')
plt.figure(figsize=(12,6))
winner_counts = df_test_info['outcome.winner'].value_counts().reset_index()
winner_counts.columns = ['Winner', 'Count']
ax = sns.barplot(data=winner_counts, x='Winner', y='Count')
plt.title("Number of Matches Won by Each Team", color = 'red')
plt.xlabel("Winning Team", color='blue', fontsize=12)
plt.ylabel("Number of Matches", color='green', fontsize=12)
plt.xticks(rotation=75)
plt.tight_layout()
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/Test_Most_Matches_Wins.png")
plt.show()

"""<h2><b><font color="gold">Batting Metrics of a Batter</font></b></h2>"""

# Basic batting metrics by batter

test_batting_stats = (
    df_test_ball.groupby('batter').agg(
        runs_scored = ('batsman_runs', 'sum'),
        balls_faced = ('batsman_runs', 'count'),
        fours = ('batsman_runs', lambda x: (x == 4).sum()),
        sixes = ('batsman_runs', lambda x: (x == 6).sum()),
        dismissals = ('is_wicket', 'sum')
    ).reset_index()
)

# Advanced metrics: high score, 50s, 100s, 200s, std, from per-innings scores

test_innings_scores = df_test_ball.groupby(['batter', 'Match_ID'])['batsman_runs'].sum().reset_index()
advanced = (
    test_innings_scores.groupby('batter').agg(
        high_score = ('batsman_runs', 'max'),
        fifties = ('batsman_runs', lambda x: ((x >= 50) & (x < 100)).sum()),
        hundreds = ('batsman_runs', lambda x: ((x >= 100) & (x < 200)).sum()),
        double_hundreds = ('batsman_runs', lambda x: (x >= 200).sum()),
        score_std = ('batsman_runs', 'std')
    ).reset_index()
)

# Round off score_std with 2 decimal values
advanced['score_std'] = advanced['score_std'].round(2)

# Merging Basic Metrics and Adavance Metrics

test_batting_stats = test_batting_stats.merge(advanced, on='batter', how='left')

# Post-processing adding strike rate, batting average to the dataframe

test_batting_stats['strike_rate'] = (test_batting_stats['runs_scored'] / test_batting_stats['balls_faced'] * 100).replace([np.inf, np.nan], 0).round(2)
test_batting_stats['batting_average'] = (test_batting_stats['runs_scored'] / test_batting_stats['dismissals']).replace([np.inf, np.nan], 0).round(2)

# Finding Recent Form from last 5 matches
N = 3
test_innings_scores['recent_form_runs'] = (
    test_innings_scores.groupby('batter')['batsman_runs']
    .transform(lambda x: x.shift(1).rolling(window=N, min_periods=1).mean())
)
test_innings_scores['career_avg_runs'] = (
    test_innings_scores.groupby('batter')['batsman_runs'].transform(lambda x: x.expanding().mean())
)

"""<h2><b><font color="gold">Metrics of Batter Vs Opponent</font></b></h2>"""

# Metrics of Batter vs Opponent

test_batter_vs_opp = (
    df_test_ball.groupby(['batter', 'opposition']).agg(
        innings_played=('Match_ID', pd.Series.nunique),
        runs_scored=('batsman_runs', 'sum'),
        balls_faced=('batsman_runs', 'count'),
        dismissals=('is_wicket', 'sum')
    ).reset_index()
)
test_batter_vs_opp['batting_average'] = (test_batter_vs_opp['runs_scored'] /test_batter_vs_opp['dismissals']).replace([np.inf, np.nan], 0).round(2)
test_batter_vs_opp['strike_rate'] = (test_batter_vs_opp['runs_scored'] / test_batter_vs_opp['balls_faced'] * 100).replace([np.inf, np.nan], 0).round(2)

# Adding Boundary Percentage & Balls per Boundary to the dataframe

test_batting_stats['boundary_pct'] = (
    (test_batting_stats['fours'] + test_batting_stats['sixes']) / test_batting_stats['balls_faced'] * 100
).replace([np.inf, np.nan], 0).round(2)

test_batting_stats['balls_per_boundary'] = (
    test_batting_stats['balls_faced'] / (test_batting_stats['fours'] + test_batting_stats['sixes'])
).replace([np.inf, np.nan], 0).round(2)

# Wide Format Batter Overall Average Metrics and Average Vs Opponent

test_batter_vs_opp_pivot = (
    test_batter_vs_opp.pivot(index='batter', columns='opposition', values='batting_average')
    .add_prefix('avg_vs_')
    .reset_index()
)
test_batting_summary = test_batting_stats.merge(test_batter_vs_opp_pivot, on='batter', how='left')

"""<h2><b><font color="gold">Metrics of Batter Vs venue</font></b></h2>"""

# Runs for each batter at each venue

test_venue_innings_scores = (
    df_test_ball.groupby(['batter', 'venue', 'Match_ID'])['batsman_runs']
    .sum()
    .reset_index()
)

# Counting 50s and 100s at each venue for each batter

test_fifty_hundred_stats = (
    test_venue_innings_scores.groupby(['batter', 'venue'])
    .agg(
        fifties=('batsman_runs', lambda x: ((x >= 50) & (x < 100)).sum()),
        hundreds=('batsman_runs', lambda x: ((x >= 100) & (x < 200)).sum()),
        double_hundreds=('batsman_runs', lambda x: (x >= 200).sum())
    )
    .reset_index()
)

# Core Batting Stats by batter and venue

test_batter_vs_venue = (
    df_test_ball.groupby(['batter', 'venue']).agg(
        innings_played=('Match_ID', pd.Series.nunique),
        runs_scored=('batsman_runs', 'sum'),
        balls_faced=('batsman_runs', 'count'),
        dismissals=('is_wicket', 'sum'),
        fours=('batsman_runs', lambda x: (x == 4).sum()),
        sixes=('batsman_runs', lambda x: (x == 6).sum())
    ).reset_index()
)

# Adding 50s/100s/200s to the venue stats df

test_batter_vs_venue = test_batter_vs_venue.merge(
    test_fifty_hundred_stats, on=['batter', 'venue'], how='left'
)

# Advanced Batting Metrics

# Finding Batting Average for each venue
test_batter_vs_venue['batting_average'] = (
    test_batter_vs_venue['runs_scored'] / test_batter_vs_venue['dismissals']
).replace([np.inf, np.nan], 0).round(2)

# Finding Strike Rate for each venue

test_batter_vs_venue['strike_rate'] = (
    test_batter_vs_venue['runs_scored'] / test_batter_vs_venue['balls_faced'] * 100
).replace([np.inf, np.nan], 0).round(2)

# Filling NaN for fifties/hundreds/double_hundreds-
test_batter_vs_venue[['fifties', 'hundreds', 'double_hundreds']] = test_batter_vs_venue[[
    'fifties', 'hundreds', 'double_hundreds'
]].fillna(0).astype(int)

"""<h2><b><font color="gold">Bowling Metrics for Bowler</font></b></h2>"""

# Calculating Maiden overs for the bowler
test_overs_summary = (
    df_test_ball.groupby(['bowler', 'Match_ID', 'batting_team', 'over'])
    .agg(total_runs_in_over=('total_runs', 'sum'))
    .reset_index()
)
test_overs_summary['is_maiden'] = (test_overs_summary['total_runs_in_over'] == 0).astype(int)

test_maidens_by_bowler = (
    test_overs_summary.groupby('bowler')['is_maiden']
    .sum()
    .reset_index()
    .rename(columns={'is_maiden': 'maidens'})
)

# Bowling Metrics for bowler

test_agg_dict = {
    'balls_bowled': ('bowler', 'count'),
    'runs_conceded': ('total_runs', 'sum'),
    'wickets': ('is_wicket', 'sum'),
    'fours_conceded': ('batsman_runs', lambda x: (x == 4).sum()),
    'sixes_conceded': ('batsman_runs', lambda x: (x == 6).sum()),
}

test_bowling_stats = df_test_ball.groupby('bowler').agg(**test_agg_dict).reset_index()
test_bowling_stats = test_bowling_stats.merge(test_maidens_by_bowler, on='bowler', how='left')
test_bowling_stats['maidens'] = test_bowling_stats['maidens'].fillna(0).astype(int)

# Per-Innings Bowling Summary

test_innings_bowling = (
    df_test_ball.groupby(['bowler', 'Match_ID', 'batting_team'])
    .agg(
        runs_conceded=('total_runs', 'sum'),
        wickets=('is_wicket', 'sum'),
        balls_bowled=('bowler', 'count')
    ).reset_index()
)

# Best innings figures (most wickets and then fewest runs)

test_best_innings = (
    test_innings_bowling.loc[
        test_innings_bowling.groupby('bowler')['wickets'].idxmax()
    ][['bowler', 'wickets', 'runs_conceded']]
    .rename(columns={'wickets': 'best_innings_wickets', 'runs_conceded': 'best_innings_runs_conceded'})
)

# 5WI, std dev
advanced_bowl = (
    test_innings_bowling.groupby('bowler').agg(
        five_wicket_hauls=('wickets', lambda x: (x >= 5).sum()),
        wickets_std=('wickets', 'std')
    ).reset_index()
)
advanced_bowl['wickets_std'] = advanced_bowl['wickets_std'].replace([np.inf, np.nan], 0).round(2)

# 10-wicket matches: sum wickets by match, count where greater than or equal to 10
test_ten_wicket_matches = (
    test_innings_bowling.groupby(['bowler', 'Match_ID'])['wickets'].sum().reset_index()
)
test_ten_wicket_count = (
    test_ten_wicket_matches.groupby('bowler')['wickets'].apply(lambda x: (x >= 10).sum()).reset_index().rename(columns={'wickets': 'ten_wicket_matches'})
)

# Merging advanced metrics
advanced_bowl = advanced_bowl.merge(test_best_innings, on='bowler', how='left')
advanced_bowl = advanced_bowl.merge(test_ten_wicket_count, on='bowler', how='left')

# Merging Basic and Advanced metrics
test_bowling_stats = test_bowling_stats.merge(advanced_bowl, on='bowler', how='left')
test_bowling_stats['best_innings_wickets'] = test_bowling_stats['best_innings_wickets'].fillna(0).astype(int)

# Post-Processing Metrics: Bowling Average, Strike Rate, Economy

# Finding Bowling Average

test_bowling_stats['bowling_average'] = (
    test_bowling_stats['runs_conceded'] / test_bowling_stats['wickets']
).replace([np.inf, np.nan], 0).round(2)


# Finding Bowler Strike rate
test_bowling_stats['strike_rate'] = (
    test_bowling_stats['balls_bowled'] / test_bowling_stats['wickets']
).replace([np.inf, np.nan], 0).round(2)

# Finding Bowler Economy
test_bowling_stats['economy'] = (
    test_bowling_stats['runs_conceded'] / test_bowling_stats['balls_bowled'] * 6
).replace([np.inf, np.nan], 0).round(2)

# Finding Wicket per balls
test_bowling_stats['balls_per_wicket'] = (
    test_bowling_stats['balls_bowled'] / test_bowling_stats['wickets']
).replace([np.inf, np.nan], 0).round(2)

#Finding Boundary Percentage
test_bowling_stats['boundary_pct'] = (
    (test_bowling_stats['fours_conceded'] + test_bowling_stats['sixes_conceded']) /
    test_bowling_stats['balls_bowled'] * 100
).replace([np.inf, np.nan], 0).round(2)

"""<h2><b><font color="gold">Bowler Vs Opponent Analysis</font></b></h2>"""

# Bowler vs Opposition Analysis

test_bowler_vs_opp = (
    df_test_ball.groupby(['bowler', 'opposition']).agg(
        matches=('Match_ID', pd.Series.nunique),
        balls_bowled=('bowler', 'count'),
        runs_conceded=('total_runs', 'sum'),
        wickets=('is_wicket', 'sum'),
    ).reset_index()
)

# Adding Bowling_Avg, Balls_Bowled, Economy fields to the dataframe
test_bowler_vs_opp['bowling_average'] = (
    test_bowler_vs_opp['runs_conceded'] / test_bowler_vs_opp['wickets']
).replace([np.inf, np.nan], 0).round(2)
test_bowler_vs_opp['strike_rate'] = (
    test_bowler_vs_opp['balls_bowled'] / test_bowler_vs_opp['wickets']
).replace([np.inf, np.nan], 0).round(2)
test_bowler_vs_opp['economy'] = (
    test_bowler_vs_opp['runs_conceded'] / test_bowler_vs_opp['balls_bowled'] * 6
).replace([np.inf, np.nan], 0).round(2)

# Wide Format (Pivot for Averages by Opposition)
test_bowler_vs_opp_pivot = (
    test_bowler_vs_opp.pivot(index='bowler', columns='opposition', values='bowling_average')
    .add_prefix('avg_vs_')
    .reset_index()
)
test_bowling_summary = test_bowling_stats.merge(test_bowler_vs_opp_pivot, on='bowler', how='left')

"""<h2><b><font color="gold"> Bowler Vs venue Analysis</font></b></h2>"""

# Bowler Vs venue
test_bowler_vs_venue = (
    df_test_ball.groupby(['bowler', 'venue']).agg(
        matches=('Match_ID', pd.Series.nunique),
        balls_bowled=('bowler', 'count'),
        runs_conceded=('total_runs', 'sum'),
        wickets=('is_wicket', 'sum'),
        fours_conceded=('batsman_runs', lambda x: (x == 4).sum()),
        sixes_conceded=('batsman_runs', lambda x: (x == 6).sum())
    ).reset_index()
)

# Adding Bowling_Avg, Strike rate, Economy fields to the dataframe
test_bowler_vs_venue['bowling_average'] = (
    test_bowler_vs_venue['runs_conceded'] / test_bowler_vs_venue['wickets']
).replace([np.inf, np.nan], 0).round(2)
test_bowler_vs_venue['strike_rate'] = (
    test_bowler_vs_venue['balls_bowled'] / test_bowler_vs_venue['wickets']
).replace([np.inf, np.nan], 0).round(2)
test_bowler_vs_venue['economy'] = (
    test_bowler_vs_venue['runs_conceded'] / test_bowler_vs_venue['balls_bowled'] * 6
).replace([np.inf, np.nan], 0).round(2)

# Finding Recent Form from last 5 matches
N = 5
test_innings_bowling['recent_form_wickets'] = (
    test_innings_bowling.groupby('bowler')['wickets']
    .transform(lambda x: x.shift(1).rolling(window=N, min_periods=1).mean()))

test_innings_bowling['career_avg_wickets'] = (
    test_innings_bowling.groupby('bowler')['wickets'].transform(lambda x: x.expanding().mean()))

"""***Importing All The Required Libraries***"""

# Importing all the required Libraries
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, r2_score
import joblib

"""<h2><b><font color="gold"> Preparing Batting Data for Modeling</font></b></h2>

***Base rows: one per (batter, Match_ID)***
"""

# Select useful match-level columns for each batter
base_cols = [
    'Match_ID','batter','venue','opposition','season','city',
    'match_type','event.match_number','outcome.winner',
    'outcome.by.runs','outcome.by.wickets','outcome.result','outcome.summary','player_id'
]
test_bat_model_df = df_test_ball[base_cols].drop_duplicates()

"""***Merge career stats***"""

# Add overall career stats for each batter
career = test_batting_stats.copy()
career = career.rename(columns={c: f"{c}_career" for c in career.columns if c != 'batter'})
test_bat_model_df = test_bat_model_df.merge(career, on='batter', how='left')

"""***Merge innings scores***"""

# Adds per-match runs (target variable) and form averages
inn = test_innings_scores[['batter','Match_ID','batsman_runs','recent_form_runs','career_avg_runs']].copy()
test_bat_model_df = test_bat_model_df.merge(inn, on=['batter','Match_ID'], how='left')

"""Merge batter vs opponent stats"""

# Add batting record of each batter against every opposition
vs_opp = test_batter_vs_opp.copy()
vs_opp = vs_opp.rename(columns={c: f"{c}_vs_opp" for c in vs_opp.columns if c not in ['batter','opposition']})
test_bat_model_df = test_bat_model_df.merge(vs_opp, on=['batter','opposition'], how='left')

"""***Merge batter vs venue stats***"""

# Add batting record of each batter at every venue
vs_venue = test_batter_vs_venue.copy()
vs_venue = vs_venue.rename(columns={c: f"{c}_vs_venue" for c in vs_venue.columns if c not in ['batter','venue']})
test_bat_model_df = test_bat_model_df.merge(vs_venue, on=['batter','venue'], how='left')

"""***Merge batting summary***"""

# Add overall summary stats (averages, boundaries, high scores etc.)
summary = test_batting_summary.copy()
summary = summary.rename(columns={c: f"{c}_summary" for c in summary.columns if c != 'batter'})
test_bat_model_df = test_bat_model_df.merge(summary, on='batter', how='left')

"""***Venueâ€“opposition aggregates***"""

# Build extra stats: innings played, runs, balls, dismissals, 50s, 100s, batting average
tmp = df_test_ball[['batter','venue','opposition','Match_ID','batsman_runs','ball_number','wicket.player_out']].copy()
tmp['is_batter_out'] = (tmp['wicket.player_out'] == tmp['batter']).astype(int)

venue_opp = tmp.groupby(['batter','venue','opposition']).agg(
    innings_played_venue_opp=('Match_ID','nunique'),
    runs_scored_venue_opp=('batsman_runs','sum'),
    balls_faced_venue_opp=('ball_number','count'),
    dismissals_venue_opp=('is_batter_out','sum')
).reset_index()

# Count 50s and 100s
match_runs = tmp.groupby(['batter','venue','opposition','Match_ID'])['batsman_runs'].sum().reset_index()
f50  = match_runs.assign(is50 =(match_runs['batsman_runs']>=50 ).astype(int)) \
                 .groupby(['batter','venue','opposition'])['is50'].sum().reset_index(name='fifties_venue_opp')
f100 = match_runs.assign(is100=(match_runs['batsman_runs']>=100).astype(int)) \
                 .groupby(['batter','venue','opposition'])['is100'].sum().reset_index(name='hundreds_venue_opp')

venue_opp = venue_opp.merge(f50,  on=['batter','venue','opposition'], how='left')
venue_opp = venue_opp.merge(f100, on=['batter','venue','opposition'], how='left')

# Simple batting average formula
venue_opp['batting_average_venue_opp'] = np.where(
    venue_opp['dismissals_venue_opp'] > 0,
    venue_opp['runs_scored_venue_opp'] / venue_opp['dismissals_venue_opp'],
    venue_opp['runs_scored_venue_opp']
)

test_bat_model_df = test_bat_model_df.merge(venue_opp, on=['batter','venue','opposition'], how='left')
print("After venueâ€“opp:", test_bat_model_df.shape)

"""***Match order + recent runs form***"""

# Add batting order of matches per player
ord_df = df_test_ball[['batter','Match_ID','dates','season','event.match_number']].drop_duplicates().copy()
ord_df['dates'] = pd.to_datetime(ord_df['dates'], errors='coerce')
ord_df = ord_df.sort_values(['batter','dates','season','event.match_number','Match_ID'])
ord_df['Match_ID_order'] = ord_df.groupby('batter').cumcount() + 1
test_bat_model_df = test_bat_model_df.merge(ord_df[['batter','Match_ID','Match_ID_order']], on=['batter','Match_ID'], how='left')
print("After match order:", test_bat_model_df.shape)

# Rolling averages of runs (recent form windows: 3, 5, 10)
test_bat_model_df = test_bat_model_df.sort_values(['batter','Match_ID_order']).copy()
for w in [3, 5, 10]:
    test_bat_model_df[f'recent_runs_mean_{w}'] = (
        test_bat_model_df.groupby('batter')['batsman_runs'].shift(1).rolling(w, min_periods=1).mean()
    )
print("After recent runs:", test_bat_model_df.shape)

"""***Clean missing values and encode categoricals***"""

# Remove duplicate columns if any
test_bat_model_df = test_bat_model_df.loc[:, ~test_bat_model_df.columns.duplicated()].copy()

# Fill missing values
for c in test_bat_model_df.select_dtypes(include='object').columns:
    test_bat_model_df[c] = test_bat_model_df[c].fillna('Unknown')
for c in test_bat_model_df.select_dtypes(include=[np.number]).columns:
    test_bat_model_df[c] = test_bat_model_df[c].fillna(test_bat_model_df[c].median())

# Define target and features
y = test_bat_model_df['batsman_runs']
drop_cols = ['batsman_runs','Match_ID','batter','player_id']
drop_cols = [c for c in drop_cols if c in test_bat_model_df.columns]
X = test_bat_model_df.drop(columns=drop_cols)

encoders = {}
# Encode categorical columns directly into X
for c in X.select_dtypes(include='object').columns:
    le = LabelEncoder()
    X[c] = le.fit_transform(X[c].astype(str))
    encoders[c] = le

print("After clean + encode:", X.shape)
print("Batter â€“ total nulls in X after fill:", int(X.isnull().sum().sum()))

joblib.dump(encoders, "label_encoders_test_batters.joblib")

maps = {c: {cls: int(i) for i, cls in enumerate(le.classes_)} for c, le in encoders.items()}
joblib.dump(maps, "label_maps_test_batters.joblib")

test_bat_model_df.to_csv("test_bat_model_df.csv", index=False)

# Cross checking any object columns before model training
object_cols = list(X.select_dtypes(include=['object']).columns)
print("Columns to be label encoded:", object_cols)

"""***Splitting Data for Model Training***"""

# Spliting data with all the columns
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""<h2><b><font color="gold"> Random Forest Regressor</font></b></h2>"""

# Training Random Forest Model
# This code trains a Random Forest model to predict runs, and prints out how well the model performed using MAE and R2 metrics
rf_test_bat = RandomForestRegressor(
    n_estimators=200, max_depth=10, random_state=42, n_jobs=-1
)
rf_test_bat.fit(X_train, y_train)

# Predicting
y_pred = rf_test_bat.predict(X_test)

# Round predicted values
y_pred_rounded = np.round(y_pred).astype(int)

# Evaluate on rounded predictions
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)

# Print results
print(f"Random Forest MAE (Rounded): {mae:.2f}")
print(f"Random Forest R2 (Rounded): {r2:.3f}")

# Comparing Actual Runs and Predicted Runs

comparison_df = pd.DataFrame({
    'Actual Runs': y_test.values,
    'Predicted Runs': np.round(y_pred).astype(int)
})
# Show the first 20 rows for a quick check
print(comparison_df.head(20))

# Plotting the top 20 feature importances as a bar chart from Random Forest Regressor
importances = rf_test_bat.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(12,5))
plt.title("Feature Importance (Random Forest)")
plt.bar(range(20), importances[indices][:20])
plt.xticks(range(20), [X.columns[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold"> Linear Regression</font></b></h2>"""

# Training Linear Regression Model
#This code trains Linear Regression model to predict runs, and prints out how well the model performed using MAE and R2 metrics
lr_test_bat = LinearRegression()
lr_test_bat.fit(X_train, y_train)

# Predicting
y_pred = lr_test_bat.predict(X_test)
# Round predicted values
y_pred_rounded = np.round(y_pred).astype(int)
# Evaluate on rounded predictions
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)
# Print results
print(f"Linear Regression MAE (Rounded): {mae:.2f}")
print(f"Linear Regression R2 (Rounded): {r2:.3f}")

# Comparing Actual Runs and Predicted Runs
comparison_df = pd.DataFrame({
    'Actual Runs': y_test.values,
    'Predicted Runs': np.round(y_pred).astype(int)
})
# Show the first 20 rows for a quick check
print(comparison_df.head(20))

# Get absolute coefficients as feature importance
coefs = np.abs(lr_test_bat.coef_)
indices = np.argsort(coefs)[::-1]
# Plot the top 20 features
plt.figure(figsize=(12, 5))
plt.title("Feature Importance (Linear Regression)")
plt.bar(range(20), coefs[indices][:20])
plt.xticks(range(20), [X.columns[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">XGBoost</font></b></h2>"""

# Training XGBoost Model
#This code trains a XGBoost model to predict runs, and prints out how well the model performed using MAE and R2 metrics
import xgboost as xgb
xgb_reg_test_bat = xgb.XGBRegressor(
    n_estimators=100,       # Number of trees
    learning_rate=0.1,      # Step size shrinkage
    max_depth=6,            # Depth of trees
    subsample=0.8,          # Row sampling
    colsample_bytree=0.8,   # Feature sampling
    random_state=42,
    n_jobs=-1               # Use all CPU cores
)
xgb_reg_test_bat.fit(X_train, y_train)

# Predict using XGBoost
y_pred = xgb_reg_test_bat.predict(X_test)
# Round predicted values
y_pred_rounded = np.round(y_pred).astype(int)
# Evaluate with rounded predictions
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)
# Print results
print(f"XGBoost MAE (Rounded): {mae:.2f}")
print(f"XGBoost R2 (Rounded): {r2:.3f}")

# Comparing Actual Runs and Predicted Runs
comparison_df = pd.DataFrame({
    'Actual Runs': y_test.values,
    'Predicted Runs': np.round(y_pred).astype(int)})
# Show the first 20 rows for a quick check
print(comparison_df.head(20))

# Get feature importances from the XGBoost model
importances = xgb_reg_test_bat.feature_importances_
indices = np.argsort(importances)[::-1]
# Plot the top 20 features
plt.figure(figsize=(12, 5))
plt.title("Feature Importance (XGBoost)")
plt.bar(range(20), importances[indices][:20])
plt.xticks(range(20), [X.columns[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">LightGBM</font></b></h2>"""

# Training LightGBM Model
#This code trains a LightGBM model to predict runs, and prints out how well the model performed using MAE and R2 metrics
import lightgbm as lgb

X.columns = X.columns.str.replace(' ', '_')

lgb_reg_test_bat = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=7,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbose=-1,
    n_jobs=-1)
lgb_reg_test_bat.fit(X_train, y_train)

# Predict using LightGBM
y_pred = lgb_reg_test_bat.predict(X_test)
# Round predicted values
y_pred_rounded = np.round(y_pred).astype(int)
# Evaluate with rounded predictions
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)
# Print results
print(f"LightGBM MAE (Rounded): {mae:.2f}")
print(f"LightGBM R2 (Rounded): {r2:.3f}")

# Comparing Actual Runs and Predicted Runs
comparison_df = pd.DataFrame({
    'Actual Runs': y_test.values,
    'Predicted Runs': np.round(y_pred).astype(int)})
# Show the first 20 rows for a quick check
print(comparison_df.head(20))

# Get feature importances
importances = lgb_reg_test_bat.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot the top 20 important features
plt.figure(figsize=(12, 5))
plt.title("Feature Importance (LightGBM)")
plt.bar(range(20), importances[indices][:20])
plt.xticks(range(20), [X.columns[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">Standardization</font></b></h2>"""

# Fix column names before scaling
X.columns = X.columns.str.replace(' ', '_')
# Standardizing features and preserving column names
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
# Train-test split
X_train_std, X_test_std, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42)

# Training and evaluating models
# Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_std, y_train)
y_pred_rf = np.round(rf.predict(X_test_std)).astype(int)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest (Rounded): MAE: {mae_rf:.3f} R2: {r2_rf:.3f}")
# Linear Regression
lr = LinearRegression()
lr.fit(X_train_std, y_train)
y_pred_lr = np.round(lr.predict(X_test_std)).astype(int)
mae_lr_test_bat = mean_absolute_error(y_test, y_pred_lr)
r2_lr_test_bat = r2_score(y_test, y_pred_lr)
print(f"Linear Regression (Rounded): MAE: {mae_lr_test_bat:.3f} R2: {r2_lr_test_bat:.3f}")
# XGBoost
xgb_reg = XGBRegressor(random_state=42, verbosity=0)
xgb_reg.fit(X_train_std, y_train)
y_pred_xgb = np.round(xgb_reg.predict(X_test_std)).astype(int)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost (Rounded): MAE: {mae_xgb:.3f} R2: {r2_xgb:.3f}")
# LightGBM
lgb_reg = LGBMRegressor(random_state=42)
lgb_reg.fit(X_train_std, y_train)
y_pred_lgb = np.round(lgb_reg.predict(X_test_std)).astype(int)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)
print(f"LightGBM (Rounded): MAE: {mae_lgb:.3f} R2: {r2_lgb:.3f}")

"""<h2><b><font color="gold">Normalization</font></b></h2>"""

# Normalizing and preserving feature names
scaler_norm = MinMaxScaler()
X_norm = pd.DataFrame(scaler_norm.fit_transform(X), columns=X.columns)
# Train-test split
X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X_norm, y, test_size=0.2, random_state=42)

# Training and evaluating models
#  Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_norm, y_train_norm)
y_pred_rf = np.round(rf.predict(X_test_norm)).astype(int)
mae_rf = mean_absolute_error(y_test_norm, y_pred_rf)
r2_rf = r2_score(y_test_norm, y_pred_rf)
print(f"Random Forest (Rounded): MAE: {mae_rf:.3f} R2: {r2_rf:.3f}")
#  Linear Regression
lr = LinearRegression()
lr.fit(X_train_norm, y_train_norm)
y_pred_lr = np.round(lr.predict(X_test_norm)).astype(int)
mae_lr = mean_absolute_error(y_test_norm, y_pred_lr)
r2_lr = r2_score(y_test_norm, y_pred_lr)
print(f"Linear Regression (Rounded): MAE: {mae_lr_test_bat:.3f} R2: {r2_lr:.3f}")

#  XGBoost
xgb_reg = XGBRegressor(random_state=42, verbosity=0)
xgb_reg.fit(X_train_norm, y_train_norm)
y_pred_xgb = np.round(xgb_reg.predict(X_test_norm)).astype(int)
mae_xgb = mean_absolute_error(y_test_norm, y_pred_xgb)
r2_xgb = r2_score(y_test_norm, y_pred_xgb)
print(f"XGBoost (Rounded): MAE: {mae_xgb:.3f} R2: {r2_xgb:.3f}")
#  LightGBM
lgb_reg = LGBMRegressor(random_state=42)
lgb_reg.fit(X_train_norm, y_train_norm)
y_pred_lgb = np.round(lgb_reg.predict(X_test_norm)).astype(int)
mae_lgb = mean_absolute_error(y_test_norm, y_pred_lgb)
r2_lgb = r2_score(y_test_norm, y_pred_lgb)
print(f"LightGBM (Rounded): MAE: {mae_lgb:.3f} R2: {r2_lgb:.3f}")

"""***LightGBM Hyperparameter Tuning***"""

# Defining the parameter grid
param_dist = {
    'num_leaves': [15, 31, 63, 127],
    'max_depth': [-1, 5, 10, 15],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 300, 500],
    'min_child_samples': [5, 10, 20, 30],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
    }

X.columns = X.columns.str.replace(' ', '_')

#  Initialize Base model
lgbm = LGBMRegressor(random_state=42, verbose=-1)
# RandomizedSearchCV
random_search_test_bat = RandomizedSearchCV(
    estimator=lgbm,
    param_distributions=param_dist,
    n_iter=30,
    scoring='neg_mean_absolute_error',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1)
# Fit the random search
random_search_test_bat.fit(X_train, y_train)

# Evaluate best model (rounded predictions)
best_lgbm_test_bat = random_search_test_bat.best_estimator_
y_pred = np.round(best_lgbm_test_bat.predict(X_test)).astype(int)
mae = mean_absolute_error(y_test, y_pred)
r2  = r2_score(y_test, y_pred)
print("Best Parameters:", random_search_test_bat.best_params_)
print(f"Tuned LightGBM (Rounded): MAE: {mae:.3f}, RÂ²: {r2:.3f}")

results = pd.DataFrame({
    'Actual': y_test.values.astype(int),
    'Predicted (rounded)': y_pred.astype(int)})
results['Error'] = results['Predicted (rounded)'] - results['Actual']
print("Actual vs Predicted (first 20 rows):")
print(results.head(20))

# Use your tuned model from RandomizedSearchCV
importances_lgb = pd.Series(best_lgbm_test_bat.feature_importances_, index=X.columns)
importances_lgb_sorted = importances_lgb.sort_values(ascending=False).head(20)

plt.figure(figsize=(10, 6))
importances_lgb_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (Tuned LightGBM) of Test Format")
plt.gca().invert_yaxis()
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">SHAP explanations for tuned LightGBM </font></b></h2>"""

import shap

# Use a small sample of X_test for speed (uses all if smaller than 1000)
X_explain = X_test.sample(n=min(1000, len(X_test)), random_state=42)

# Build explainer and compute SHAP values
explainer = shap.TreeExplainer(best_lgbm_test_bat)
shap_values = explainer.shap_values(X_explain)

"""***SHAP Beeswarm Plot***"""

# SHAP beeswarm showing impact of each feature on individual predictions
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,
    X_explain,
    max_display=10,
    show=False
)
plt.title("SHAP value (impact on model output)")
plt.tight_layout()
plt.show()

"""***SHAP Bar Plot***

"""

# SHAP bar chart showing average absolute impact of each feature
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,
    X_explain,
    plot_type="bar",
    max_display=10,
    show=False
)
plt.title("mean(|SHAP value|) â€“ average impact on model output")
plt.xlabel("mean(|SHAP value|) (average impact on model output)")
plt.tight_layout()
plt.show()

"""***SHAP Single-Feature Dependence Plot***"""

# SHAP dependence plot showing how one featureâ€™s values affect predictions

# Pick the top feature by mean absolute SHAP value
mean_abs = np.abs(shap_values).mean(axis=0)
top_idx = np.argsort(mean_abs)[-1]
top_feat = X_explain.columns[top_idx]   # e.g., "vs_venue_wickets"

plt.figure(figsize=(10, 6))
shap.dependence_plot(
    top_feat,          # feature to visualize
    shap_values,       # SHAP values from explainer
    X_explain,         # data sample used for explanations
    show=False
)
plt.title(f"SHAP Dependence Plot â€“ {top_feat}")
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">Preparing Data For Bowler Model Training</font></b></h2>

***Copy base dataframe & fill extras***
"""

# Make a working copy
test_df = df_test_ball.copy()

# Fill missing extras with 0
test_df['extras.wides']   = test_df['extras.wides'].fillna(0)
test_df['extras.noballs'] = test_df['extras.noballs'].fillna(0)
test_df['extras.byes']    = test_df['extras.byes'].fillna(0)
test_df['extras.legbyes'] = test_df['extras.legbyes'].fillna(0)

"""***Create per-ball helper columns***"""

# Mark legal deliveries (not wide and not no-ball)
test_df['is_legal_ball'] = ((test_df['extras.wides'] == 0) &
                            (test_df['extras.noballs'] == 0)).astype(int)

# Runs conceded per ball = batsman runs + wides + no-balls
test_df['runs_conceded_ball'] = (
    test_df['batsman_runs'].fillna(0) +
    test_df['extras.wides'] +
    test_df['extras.noballs']
)

# Bowler extras = wides + no-balls (byes/legbyes not charged to bowler)
test_df['bowler_extras_ball'] = test_df['extras.wides'] + test_df['extras.noballs']

"""***Calculate runs conceded per over***"""

# Total runs conceded in each over for each bowler in each match
over_runs = (
    test_df.groupby(['Match_ID', 'bowler', 'over'])['runs_conceded_ball']
           .sum()
           .reset_index(name='runs_in_over'))

"""***Identify maiden overs per bowler per match***"""

# Mark maiden overs (0 runs conceded in an over)
over_runs['is_maiden_over'] = (over_runs['runs_in_over'] == 0).astype(int)

# Count maiden overs for each bowler in each match
maidens = (
    over_runs.groupby(['Match_ID', 'bowler'])['is_maiden_over']
             .sum()
             .reset_index(name='maidens_in_match'))

"""***Build match-level aggregates + add opposition & metadata***

"""

# Totals per (Match_ID, bowler)
agg = (
    test_df.groupby(['Match_ID', 'bowler'])
           .agg({
               'is_legal_ball': 'sum',
               'runs_conceded_ball': 'sum',
               'is_wicket': 'sum',
               'bowler_extras_ball': 'sum'
           })
           .reset_index()
           .rename(columns={
               'is_legal_ball': 'balls_bowled_in_match',
               'runs_conceded_ball': 'runs_conceded_in_match',
               'is_wicket': 'wickets_in_match',
               'bowler_extras_ball': 'extras_in_match'
           })
)

# Economy (runs per over) and Strike Rate (balls per wicket)
agg['econ_rate_in_match']   = agg['runs_conceded_in_match'] / (agg['balls_bowled_in_match'] / 6)
agg['strike_rate_in_match'] = agg['balls_bowled_in_match'] / agg['wickets_in_match'].replace(0, np.nan)

# Opposition = batting_team faced most balls in that match
opp = (
    test_df.groupby(['Match_ID', 'bowler', 'batting_team'])
           .size()
           .reset_index(name='balls_against_team')
    .sort_values(['Match_ID','bowler','balls_against_team'], ascending=[True, True, False])
    .drop_duplicates(['Match_ID','bowler'])
    .rename(columns={'batting_team': 'opposition'})[['Match_ID','bowler','opposition']]
)

# Match metadata (first row per match)
meta = (
    test_df.groupby('Match_ID')[[
        'venue','season','match_type','outcome.winner','outcome.by.runs',
        'outcome.by.wickets','outcome.result','outcome.summary','dates'
    ]].first().reset_index()
    .rename(columns={'dates': 'match_date'})
)

# Combine all parts
test_bowl_model_df = (
    agg.merge(maidens, on=['Match_ID','bowler'], how='left')
       .merge(opp,     on=['Match_ID','bowler'], how='left')
       .merge(meta,    on='Match_ID',            how='left')
)
# Clean wickets to int
test_bowl_model_df['wickets_in_match'] = test_bowl_model_df['wickets_in_match'].fillna(0).astype(int)

"""***Merge career bowling stats***"""

# Keep relevant columns and prefix with career_
career_cols = ['bowler','balls_bowled','runs_conceded','wickets','fours_conceded',
               'sixes_conceded','maidens','five_wicket_hauls','wickets_std',
               'best_innings_wickets','best_innings_runs_conceded','ten_wicket_matches',
               'bowling_average','strike_rate','economy','balls_per_wicket','boundary_pct']
career_df = test_bowling_stats[career_cols].rename(
    columns={c: f'career_{c}' for c in career_cols if c != 'bowler'}
    )

# Merge into match-level df
test_bowl_model_df = test_bowl_model_df.merge(career_df, on='bowler', how='left')

"""***Merge opposition-level stats***"""

# Select and rename vs-opp columns
opp_df = test_bowler_vs_opp[[
    'bowler','opposition','matches','balls_bowled','runs_conceded','wickets',
    'bowling_average','strike_rate','economy'
]].rename(columns={
    'matches':'vs_opp_matches',
    'balls_bowled':'vs_opp_balls_bowled',
    'runs_conceded':'vs_opp_runs_conceded',
    'wickets':'vs_opp_wickets',
    'bowling_average':'vs_opp_bowling_average',
    'strike_rate':'vs_opp_strike_rate',
    'economy':'vs_opp_economy'
})
# Merge on (Match_ID, bowler)'s opposition
test_bowl_model_df = test_bowl_model_df.merge(
    opp_df, on=['bowler','opposition'], how='left')

"""***Merge venue-level stats***"""

# Select and rename vs-venue columns
venue_df = test_bowler_vs_venue[[
    'bowler','venue','matches','balls_bowled','runs_conceded','wickets',
    'fours_conceded','sixes_conceded','bowling_average','strike_rate','economy'
]].rename(columns={
    'matches':'vs_venue_matches',
    'balls_bowled':'vs_venue_balls_bowled',
    'runs_conceded':'vs_venue_runs_conceded',
    'wickets':'vs_venue_wickets',
    'fours_conceded':'vs_venue_fours_conceded',
    'sixes_conceded':'vs_venue_sixes_conceded',
    'bowling_average':'vs_venue_bowling_average',
    'strike_rate':'vs_venue_strike_rate',
    'economy':'vs_venue_economy'
})

# Merge on (bowler, venue)
test_bowl_model_df = test_bowl_model_df.merge(
    venue_df, on=['bowler','venue'], how='left'
)

"""***Merge innings-level stats***"""

# Sum innings to match level and merge
ib_match = (
    test_innings_bowling
    .groupby(['Match_ID','bowler'], as_index=False)
    .agg({
        'runs_conceded':'sum',
        'wickets':'sum',
        'balls_bowled':'sum',
        'recent_form_wickets':'first',
        'career_avg_wickets':'first',
        'batting_team':'first'
    })
    .rename(columns={
        'runs_conceded':'innings_runs_conceded',
        'wickets':'innings_wickets',
        'balls_bowled':'innings_balls_bowled',
        'batting_team':'innings_batting_team'
    })
)

test_bowl_model_df = test_bowl_model_df.merge(ib_match, on=['Match_ID','bowler'], how='left')

"""***Merge opponent averages***"""

# Pick all average-vs-opponent columns and merge by bowler
opp_avg_cols = [c for c in test_bowling_summary.columns if c.startswith('avg_vs_')]
bowler_opp_avg = test_bowling_summary[['bowler'] + opp_avg_cols].copy()

test_bowl_model_df = test_bowl_model_df.merge(bowler_opp_avg, on='bowler', how='left')

"""***Create recent-form rolling averages***"""

# Create rolling means from previous matches (grouped by bowler)
# Uses columns added in Block 9: innings_wickets, innings_runs_conceded
windows = [3, 5, 10, 15, 20]

# Ensure base cols exist (create simple zeros if missing)
if 'innings_wickets' not in test_bowl_model_df.columns:
    test_bowl_model_df['innings_wickets'] = 0
if 'innings_runs_conceded' not in test_bowl_model_df.columns:
    test_bowl_model_df['innings_runs_conceded'] = 0

# Fill NaNs to keep the code simple
test_bowl_model_df['innings_wickets'] = test_bowl_model_df['innings_wickets'].fillna(0)
test_bowl_model_df['innings_runs_conceded'] = test_bowl_model_df['innings_runs_conceded'].fillna(0)

# Rolling means for wickets
for w in windows:
    col = f'recent_wickets_mean_{w}'
    test_bowl_model_df[col] = (
        test_bowl_model_df.groupby('bowler')['innings_wickets']
                          .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())
    )

# Rolling means for runs conceded
for w in windows:
    col = f'recent_runs_conceded_mean_{w}'
    test_bowl_model_df[col] = (
        test_bowl_model_df.groupby('bowler')['innings_runs_conceded']
                          .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())
    )

"""***Create ratio features***"""

# Compare venue & opposition performance to career numbers

# Venue vs Career
test_bowl_model_df['venue_wickets_ratio'] = (
    test_bowl_model_df['vs_venue_wickets'] / test_bowl_model_df['career_avg_wickets'].replace(0, np.nan)
)
test_bowl_model_df['venue_sr_ratio'] = (
    test_bowl_model_df['vs_venue_strike_rate'] / test_bowl_model_df['career_strike_rate'].replace(0, np.nan)
)

# Opposition vs Career
test_bowl_model_df['opp_wickets_ratio'] = (
    test_bowl_model_df['vs_opp_wickets'] / test_bowl_model_df['career_avg_wickets'].replace(0, np.nan)
)
test_bowl_model_df['opp_sr_ratio'] = (
    test_bowl_model_df['vs_opp_strike_rate'] / test_bowl_model_df['career_strike_rate'].replace(0, np.nan)
)

# Clean infinities
for c in ['venue_wickets_ratio','venue_sr_ratio','opp_wickets_ratio','opp_sr_ratio']:
    test_bowl_model_df[c] = test_bowl_model_df[c].replace([np.inf, -np.inf], np.nan)

"""***Final Clean and Label Encoding***"""

# Remove duplicate columns (if any)
test_bowl_model_df = test_bowl_model_df.loc[:, ~test_bowl_model_df.columns.duplicated()]

# Fill missing values
for c in test_bowl_model_df.select_dtypes(include='object').columns:
    test_bowl_model_df[c] = test_bowl_model_df[c].fillna('Unknown')
for c in test_bowl_model_df.select_dtypes(include=[np.number]).columns:
    test_bowl_model_df[c] = test_bowl_model_df[c].fillna(test_bowl_model_df[c].median())

# Define target
y = test_bowl_model_df['wickets_in_match']

# Final drop list (leakage-safe: target, IDs, post-match aggregates, innings stats, outcomes)
drop_cols = [
    'wickets_in_match','Match_ID','player_id','bowler','match_date',
    'balls_bowled_in_match','runs_conceded_in_match','extras_in_match',
    'maidens_in_match','econ_rate_in_match','strike_rate_in_match',
    'innings_runs_conceded','innings_wickets','innings_balls_bowled','innings_batting_team',
    'outcome.winner','outcome.by.runs','outcome.by.wickets','outcome.result','outcome.summary'
]
drop_cols = [c for c in drop_cols if c in test_bowl_model_df.columns]

# Build features
X = test_bowl_model_df.drop(columns=drop_cols)

encoders_bowlers = {}
# Label-encode categoricals
for c in X.select_dtypes(include='object').columns:
    le = LabelEncoder()
    X[c] = le.fit_transform(X[c].astype(str))
    encoders[c] = le

# Final check
print("X shape:", X.shape)
print("Total nulls in X:", int(X.isnull().sum().sum()))

joblib.dump(encoders, "label_encoders_test_bowl.joblib")

test_bowl_model_df.to_csv("test_bowl_model_df.csv", index=False)

maps = {c: {cls: int(i) for i, cls in enumerate(le.classes_)} for c, le in encoders.items()}
joblib.dump(maps, "label_maps_test_bowl.joblib")

# Cross checking any object columns before model training
object_cols = list(X.select_dtypes(include=['object']).columns)
print("Columns to be label encoded:", object_cols)

"""***Splitting Data for Model Training***"""

# Spliting data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""<h2><b><font color="gold">Random Forest</font></b></h2>"""

# Training Random Forest Model
# This code trains a Random Forest model to predict runs, and prints out how well the model performed using MAE and R2 metrics
rf_test_bowl = RandomForestRegressor(n_estimators=100, random_state=42)
rf_test_bowl.fit(X_train, y_train)

# Predict on test data
y_pred = rf_test_bowl.predict(X_test)
y_pred_rounded = np.round(y_pred).astype(int)

# Evaluation
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)

print(f"ðŸŽ¯ Random Forest (Wickets Prediction)")
print(f"MAE: {mae:.3f}")
print(f"RÂ²: {r2:.3f}")

# Create a DataFrame to compare actual and predicted values
results_df = pd.DataFrame({
    'Actual Wickets': y_test.values,
    'Predicted Wickets': y_pred_rounded
})

# Show top 20 rows
print(results_df.head(20))

# Feature importances
importances = pd.Series(rf_test_bowl.feature_importances_, index=X.columns)
importances_sorted = importances.sort_values(ascending=False)[:20]

# Plot top 20 features
plt.figure(figsize=(10, 6))
importances_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (Random Forest)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">Linear Regression</font></b></h2>"""

# Training Linear Regression Model
#This code trains Linear Regression model to predict runs, and prints out how well the model performed using MAE and R2 metrics
lr_test_bowl = LinearRegression()
lr_test_bowl.fit(X_train, y_train)

# Predict on test data
y_pred_lr = lr_test_bowl.predict(X_test)
y_pred_lr_rounded = np.round(y_pred_lr).astype(int)

# Evaluation
mae_lr = mean_absolute_error(y_test, y_pred_lr_rounded)
r2_lr = r2_score(y_test, y_pred_lr_rounded)

print("ðŸŽ¯ Linear Regression (Wickets Prediction)")
print(f"MAE: {mae_lr:.3f}")
print(f"RÂ²: {r2_lr:.3f}")

# Create a DataFrame to compare actual and predicted values
results_lr_df = pd.DataFrame({
    'Actual Wickets': y_test.values,
    'Predicted Wickets': y_pred_lr_rounded
})

# Show top 20 rows
print(results_lr_df.head(20))

# Feature importances (coefficients)
coefs = pd.Series(lr_test_bowl.coef_, index=X.columns)
coefs_sorted = coefs.sort_values(ascending=False)[:20]

# Plot top 20 coefficients
plt.figure(figsize=(10, 6))
coefs_sorted.plot(kind='barh')
plt.title("Top 20 Coefficients (Linear Regression)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">XGBoost</font></b></h2>"""

# Training XGBoost Model
#This code trains a XGBoost model to predict runs, and prints out how well the model performed using MAE and R2 metrics
xgb_test_bowl = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_test_bowl.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb_test_bowl.predict(X_test)
y_pred_xgb_rounded = np.round(y_pred_xgb).astype(int)

# Evaluation
mae_xgb = mean_absolute_error(y_test, y_pred_xgb_rounded)
r2_xgb = r2_score(y_test, y_pred_xgb_rounded)

print("ðŸŽ¯ XGBoost Regressor (Wickets Prediction)")
print(f"MAE: {mae_xgb:.3f}")
print(f"RÂ²: {r2_xgb:.3f}")

# Create a DataFrame to compare actual and predicted values
results_xgb_df = pd.DataFrame({
    'Actual Wickets': y_test.values,
    'Predicted Wickets': y_pred_xgb_rounded
})

# Show top 20 rows
print(results_xgb_df.head(20))

# Feature importances
importances_xgb = pd.Series(xgb_test_bowl.feature_importances_, index=X.columns)
importances_xgb_sorted = importances_xgb.sort_values(ascending=False)[:20]

# Plot top 20 features
plt.figure(figsize=(10, 6))
importances_xgb_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (XGBoost)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">LightGBM</font></b></h2>"""

# Training LightGBM Model
#This code trains a LightGBM model to predict runs, and prints out how well the model performed using MAE and R2 metrics
import lightgbm as lgb

X.columns = X.columns.str.replace(' ', '_')

lgb_test_bowl = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=7,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbose=-1,
    n_jobs=-1)
lgb_test_bowl.fit(X_train, y_train)

# Predict on test data
y_pred_lgb = lgb_test_bowl.predict(X_test)
y_pred_lgb_rounded = np.round(y_pred_lgb).astype(int)

# Evaluation
mae_lgb = mean_absolute_error(y_test, y_pred_lgb_rounded)
r2_lgb = r2_score(y_test, y_pred_lgb_rounded)

print("ðŸŽ¯ LightGBM Regressor (Wickets Prediction)")
print(f"MAE: {mae_lgb:.3f}")
print(f"RÂ²: {r2_lgb:.3f}")

joblib.dump(lgb_test_bowl, "lgb_test_bowl.joblib")

# Create a DataFrame to compare actual and predicted values
results_lgb_df = pd.DataFrame({
    'Actual Wickets': y_test.values,
    'Predicted Wickets': y_pred_lgb_rounded
})

# Show top 20 rows
print(results_lgb_df.head(20))

# Feature importances
importances_lgb = pd.Series(lgb_test_bowl.feature_importances_, index=X.columns)
importances_lgb_sorted = importances_lgb.sort_values(ascending=False)[:20]

# Plot top 20 features
plt.figure(figsize=(10, 6))
importances_lgb_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (LightGBM)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">Standardization</font></b></h2>"""

# Fix column names before scaling
X.columns = X.columns.str.replace(' ', '_')

# Standardizing features and preserving column names
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Train-test split
X_train_std, X_test_std, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_std, y_train)
y_pred_rf = np.round(rf.predict(X_test_std)).astype(int)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest (Rounded): MAE: {mae_rf:.3f} R2: {r2_rf:.3f}")

# Linear Regression
lr = LinearRegression()
lr.fit(X_train_std, y_train)
y_pred_lr = np.round(lr.predict(X_test_std)).astype(int)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)
print(f"Linear Regression (Rounded): MAE: {mae_lr:.3f} R2: {r2_lr:.3f}")

# XGBoost
xgb_reg = XGBRegressor(random_state=42, verbosity=0)
xgb_reg.fit(X_train_std, y_train)
y_pred_xgb = np.round(xgb_reg.predict(X_test_std)).astype(int)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost (Rounded): MAE: {mae_xgb:.3f} R2: {r2_xgb:.3f}")

# LightGBM
lgb_reg = LGBMRegressor(random_state=42)
lgb_reg.fit(X_train_std, y_train)
y_pred_lgb = np.round(lgb_reg.predict(X_test_std)).astype(int)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)
print(f"LightGBM (Rounded): MAE: {mae_lgb:.3f} R2: {r2_lgb:.3f}")

"""<h2><b><font color="gold">Normalization</font></b></h2>"""

# Fix column names
X.columns = X.columns.str.replace(' ', '_')

# Normalize features to [0, 1]
scaler = MinMaxScaler()
X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Train-test split
X_train_norm, X_test_norm, y_train, y_test = train_test_split(
    X_normalized, y, test_size=0.2, random_state=42
)

# Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_norm, y_train)
y_pred_rf = np.round(rf.predict(X_test_norm)).astype(int)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest (Rounded): MAE: {mae_rf:.3f} R2: {r2_rf:.3f}")

# Linear Regression
lr = LinearRegression()
lr.fit(X_train_norm, y_train)
y_pred_lr = np.round(lr.predict(X_test_norm)).astype(int)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)
print(f"Linear Regression (Rounded): MAE: {mae_lr:.3f} R2: {r2_lr:.3f}")

# XGBoost
xgb_reg = XGBRegressor(random_state=42, verbosity=0)
xgb_reg.fit(X_train_norm, y_train)
y_pred_xgb = np.round(xgb_reg.predict(X_test_norm)).astype(int)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost (Rounded): MAE: {mae_xgb:.3f} R2: {r2_xgb:.3f}")

# LightGBM
lgb_reg = LGBMRegressor(random_state=42)
lgb_reg.fit(X_train_norm, y_train)
y_pred_lgb = np.round(lgb_reg.predict(X_test_norm)).astype(int)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)
print(f"LightGBM (Rounded): MAE: {mae_lgb:.3f} R2: {r2_lgb:.3f}")

"""<h2><b><font color="gold">Hyper Parameter LightGBM</font></b></h2>"""

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid
param_dist = {
    'num_leaves': [20, 31, 40, 50],
    'max_depth': [5, 7, 10, 12, -1],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'min_child_samples': [5, 10, 20, 30],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

X.columns = X.columns.str.replace(' ', '_')

# Initialize model
lgbm = LGBMRegressor(random_state=42, verbose=-1)

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=lgbm,
    param_distributions=param_dist,
    n_iter=30,
    scoring='neg_mean_absolute_error',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Fit the random search
random_search.fit(X_train, y_train)

# Predicting and evaluating
best_lgbm = random_search.best_estimator_
y_pred = np.round(best_lgbm.predict(X_test)).astype(int)

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Best Parameters:", random_search.best_params_)
print(f"Tuned LightGBM (Rounded): MAE: {mae:.3f}, R2: {r2:.3f}")

results = pd.DataFrame({
    'Actual': y_test.values.astype(int),
    'Predicted (rounded)': y_pred.astype(int)
})
results['Error'] = results['Predicted (rounded)'] - results['Actual']

print("Actual vs Predicted (first 20 rows):")
print(results.head(20))

results = pd.DataFrame({
    'Actual': y_test.values.astype(int),
    'Predicted (rounded)': y_pred.astype(int)
})
results['Error'] = results['Predicted (rounded)'] - results['Actual']

print("Actual vs Predicted (first 20 rows):")
print(results.head(20))

# Use your tuned model from RandomizedSearchCV
importances_lgb = pd.Series(best_lgbm.feature_importances_, index=X.columns)
importances_lgb_sorted = importances_lgb.sort_values(ascending=False).head(20)

plt.figure(figsize=(10, 6))
importances_lgb_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (Tuned LightGBM) of Test Format")
plt.gca().invert_yaxis()
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">SHAP explanations for tuned LightGBM </font></b></h2>"""

import shap

# Use a small sample of X_test for speed (uses all if smaller than 1000)
X_explain = X_test.sample(n=min(1000, len(X_test)), random_state=42)

# Build explainer and compute SHAP values
explainer = shap.TreeExplainer(best_lgbm)
shap_values = explainer.shap_values(X_explain)

"""***SHAP Beeswarm Plot***"""

# SHAP beeswarm showing impact of each feature on individual predictions
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,
    X_explain,
    max_display=10,
    show=False
)
plt.title("SHAP value (impact on model output)")
plt.tight_layout()
plt.show()

"""***SHAP Bar Plot***

"""

# SHAP bar chart showing average absolute impact of each feature
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,
    X_explain,
    plot_type="bar",
    max_display=10,
    show=False
)
plt.title("mean(|SHAP value|) â€“ average impact on model output")
plt.xlabel("mean(|SHAP value|) (average impact on model output)")
plt.tight_layout()
plt.show()

"""***SHAP Single-Feature Dependence Plot***"""

# SHAP dependence plot showing how one featureâ€™s values affect predictions

# Pick the top feature by mean absolute SHAP value
mean_abs = np.abs(shap_values).mean(axis=0)
top_idx = np.argsort(mean_abs)[-1]
top_feat = X_explain.columns[top_idx]   # e.g., "vs_venue_wickets"

plt.figure(figsize=(10, 6))
shap.dependence_plot(
    top_feat,          # feature to visualize
    shap_values,       # SHAP values from explainer
    X_explain,         # data sample used for explanations
    show=False
)
plt.title(f"SHAP Dependence Plot â€“ {top_feat}")
plt.tight_layout()
plt.show()

