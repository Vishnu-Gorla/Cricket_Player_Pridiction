# -*- coding: utf-8 -*-
"""Copy of T20_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kqj4pj4VZrvfgN2_ObSIygDBIPoAkc75

<center>
<b><i><font color="Sky Blue" size="8">T20 Cricket Analysis</font></i></b>
</center>

<h2><b><font color="gold">Introduction</font></b></h2>
"""

# Mounting Google Drive

from google.colab import drive
drive.mount('/content/drive')

# Importing Required Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Importing warnings to ignore warnings

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Path for files from drive
T20_ball_file = '/content/drive/MyDrive/Cricket_Project/data/model_ready/Ball By Ball/t20_ball_by_ball.csv'
T20_info_file = '/content/drive/MyDrive/Cricket_Project/data/model_ready/Info/t20_info.csv'
T20_summary_file = '/content/drive/MyDrive/Cricket_Project/data/model_ready/Summary/t20_summary.csv'
T20_venue_file = '/content/drive/MyDrive/Cricket_Project/data/model_ready/T20_match_Id_cleaned_venues.csv'
players_file = '/content/drive/MyDrive/Cricket_Project/data/player_lookup_all_formats.csv'

# Loading the CSV's into a DataFrame
df_T20_ball = pd.read_csv(T20_ball_file, low_memory=False)
df_T20_info = pd.read_csv(T20_info_file, low_memory=False)
df_T20_summary = pd.read_csv(T20_summary_file, low_memory=False)
df_T20_venue = pd.read_csv(T20_venue_file, low_memory=False)
df_players_lookup = pd.read_csv(players_file, low_memory=False)

# Verifying the shape of Dataframes

print(f"Ball By Ball shape: {df_T20_ball.shape}")
print(f"Info shape: {df_T20_info.shape}")
print(f"Summary shape: {df_T20_summary.shape}")
print(f"Venue shape: {df_T20_venue.shape}")
print(f"Players shape: {df_players_lookup.shape}")

"""<h2><b><font color="gold">Data Cleaning</font></b></h2>"""

# Columns you want to keep
keep_cols = [
    'batter', 'bowler', 'non_striker', 'batting_team', 'over', 'team',
    'runs.batter', 'runs.extras', 'runs.total', 'extras.wides',
    'extras.legbyes', 'extras.byes', 'extras.noballs',
    'wicket.kind', 'wicket.player_out', 'Match Id'
]

# Keep only these columns
df_T20_ball = df_T20_ball[keep_cols]

# Checking for missing values in each column
missing_counts = df_T20_ball.isnull().sum()
print(missing_counts)

# Remove columns with all missing values

t20_cols_with_all_missing = [
    col for col in df_T20_ball.columns if df_T20_ball[col].isnull().all()
]
df_T20_ball.drop(columns=t20_cols_with_all_missing, inplace=True)

# Fill missing values in main numeric columns with 0
df_T20_ball['runs.batter'] = df_T20_ball['runs.batter'].fillna(0)
df_T20_ball['runs.extras'] = df_T20_ball['runs.extras'].fillna(0)
df_T20_ball['runs.total'] = df_T20_ball['runs.total'].fillna(0)

# Fill missing values in extras sub-columns with 0
for col in ['extras.wides', 'extras.noballs', 'extras.legbyes', 'extras.byes', 'extras.penalty']:
    if col in df_T20_ball.columns:
        df_T20_ball[col] = df_T20_ball[col].fillna(0)

# Create is_wicket column: 1 if a wicket fell on this ball, else 0
df_T20_ball['is_wicket'] = df_T20_ball['wicket.kind'].notnull().astype(int)

# Renaming columns for better readability

df_T20_ball = df_T20_ball.rename(columns={
    'runs.batter': 'batsman_runs',
    'runs.extras': 'extras',
    'runs.total': 'total_runs'
})

"""<h2><b><font color="gold">Data Pre-Processing</font></b></h2>"""

# Standardizing the Match ID Column Name

for df in [df_T20_ball, df_T20_info, df_T20_summary, df_T20_venue]:
    if 'Match Id' in df.columns:
        df.rename(columns={'Match Id': 'Match_ID'}, inplace=True)

# Removing unnesasary columns before merging

required_info_cols = [
    'Match_ID', 'balls_per_over', 'city', 'dates', 'event.name',
    'event.match_number', 'gender', 'match_type', 'match_type_number',
    'season', 'teams', 'player_of_match', 'outcome.winner', 'outcome.by.runs',
    'outcome.by.wickets', 'outcome.result', 'outcome.summary']

# Use only columns present in your file

df_T20_info = df_T20_info[[col for col in required_info_cols if col in df_T20_info.columns]].copy()

# Merging Ball data and Info data

df_T20_ball = df_T20_ball.merge(df_T20_info, on='Match_ID', how='left')

# merging Ball data and venue data
df_T20_ball = df_T20_ball.merge(df_T20_venue, on='Match_ID', how='left')

# Keep only T20 players (one row per name)
T20_players = df_players_lookup[df_players_lookup['format'] == 'T20'].drop_duplicates(subset=['name'])

# Merge player_id into your main ball-by-ball df using batter name
df_T20_ball = df_T20_ball.merge(
    T20_players[['player_id', 'name']],
    left_on='batter',
    right_on='name',
    how='left'
)

# Remove the extra name column
df_T20_ball = df_T20_ball.drop(columns=['name'])

"""<h2><b><font color="gold">Finding batting team and opponent team for better analysis</font></b></h2>"""

# Finding opponent team
''' This code defines a function to identify the opposing team for each ball,
handling cases where the team names may be stored as a list, a comma-separated string,
or a stringified list like "['India', 'Pakistan']". It then applies this function to the data,
creating a new column with the opponent team’s name.'''

def get_opposition(row):
    teams = row['teams']
    if isinstance(teams, list):
        pass
    elif isinstance(teams, str):
        s = teams.strip()
        # Handle stringified lists by removing surrounding [ ]
        if s.startswith('[') and s.endswith(']'):
            s = s[1:-1]
        # Split and strip quotes/spaces
        teams = [t.strip().strip("'").strip('"') for t in s.split(',')]

    else:
        return None

    # Safety: require exactly two teams after parsing
    if not isinstance(teams, list) or len(teams) != 2:
        return None

    bt = row['batting_team']
    if bt == teams[0]:
        return teams[1]
    elif bt == teams[1]:
        return teams[0]
    else:
        return None

df_T20_ball['opposition'] = df_T20_ball.apply(get_opposition, axis=1)
print("T20 missing opposition count:", df_T20_ball['opposition'].isna().sum())

# Finding Batting Team
if 'team' in df_T20_ball.columns:
    df_T20_ball['batting_team'] = df_T20_ball['team']
print("Missing batting_team count:", df_T20_ball['batting_team'].isna().sum())

"""<h2><b><font color="gold">Finding ball number of that over</font></b></h2>"""

# Finding ball number of that over
''' This function assigns ball numbers in the format 'over.ball' for each delivery,
 ensuring only legal balls (not wides or no-balls) increment the count within each over.
 It then adds these ball numbers as a new column in the DataFrame.'''


def assign_ball_numbers_exact(df):
    ball_numbers = []
    prev_over = None
    ball_count = 1

    for idx, row in df.iterrows():
        current_over = row['over']
        wide = row['extras.wides']
        noball = row['extras.noballs']

        # New over: reset ball count to 1
        if idx == 0 or current_over != prev_over:
            ball_count = 1

        ball_numbers.append(f"{current_over}.{ball_count}")

        # Only increment ball count if this ball is LEGAL
        if (wide == 0) and (noball == 0):
            if ball_count < 6:
                ball_count += 1
        # If not legal, ball_count does NOT increment

        prev_over = current_over

    df['ball_number'] = ball_numbers
    return df


df_T20_ball = assign_ball_numbers_exact(df_T20_ball)

"""<h2><b><font color="gold">Sorting Date Column</font></b></h2>"""

import pandas as pd

# 1) Convert 'dates' from list-like string to datetime
df_T20_ball['dates'] = (
    df_T20_ball['dates'].astype(str).str.extract(r'(\d{4}-\d{2}-\d{2})')[0]
)
df_T20_ball['dates'] = pd.to_datetime(df_T20_ball['dates'], errors='coerce')

# 2) Convert 'ball_number' to float
df_T20_ball['ball_number'] = pd.to_numeric(df_T20_ball['ball_number'], errors='coerce')

# Quick check
print(df_T20_ball[['dates', 'ball_number']].dtypes)
print(df_T20_ball[['dates', 'ball_number']].head())

"""<h2><b><font color="gold">Overall analysis visualizations</font></b></h2>

"""

# Most Runs scored Battters in T20

plt.figure(figsize=(12,5))
top_batsmen = df_T20_ball.groupby('batter')['batsman_runs'].sum().sort_values(ascending=False).head(10)
sns.barplot(
    x=top_batsmen.values,
    y=top_batsmen.index,
    hue=top_batsmen.index,
    palette="viridis",
    legend=False
)

plt.title("Top 10 Most T20 Run Scorers ", color = 'red')
plt.xlabel("Total Runs",  color='blue', fontsize=12)
plt.ylabel("Batter", color='green', fontsize=12)
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/T20_Top_10_Run_Scoreres.png")
plt.show()

# Most wicket taking bowlers in T20

plt.figure(figsize=(12,5))
top_bowlers = df_T20_ball[df_T20_ball['is_wicket']==1].groupby('bowler').size().sort_values(ascending=False).head(10)
sns.barplot(
    x=top_bowlers.values,
    y=top_bowlers.index,
    hue=top_bowlers.index,
    palette="viridis",
    legend=False
)
plt.xlabel('Wickets', color='blue', fontsize=12)
plt.ylabel('Bowler', color='green', fontsize=12)
plt.title('Top 10 T20 Bowlers by Wickets', color = 'red')
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/T20_Top_10_Wicket_Takers.png")
plt.show()

# Step 1: Calculate runs per innings
runs_per_innings = df_T20_ball.groupby(['Match_ID', 'batting_team'])['total_runs'].sum().reset_index()

# Step 2: Count number of matches played by each team
matches_count = runs_per_innings['batting_team'].value_counts()

# Step 3: Keep only teams with at least 50 matches
teams_with_50plus = matches_count[matches_count >= 100].index

# Step 4: Filter the data
filtered_runs_per_innings = runs_per_innings[runs_per_innings['batting_team'].isin(teams_with_50plus)]

# Step 5: Plot
plt.figure(figsize=(16,6))
sns.boxplot(x='batting_team', y='total_runs', data=filtered_runs_per_innings)
plt.title("Distribution of Total Runs per Match by Batting Team in T20's", color='red')
plt.xlabel("Batting Team", color='blue', fontsize=12)
plt.ylabel("Runs", color='green', fontsize=12)
plt.xticks(rotation=90)

# Save the plot
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/T20_Overall_Runs_ByTeam.png", dpi=300, bbox_inches='tight')

plt.show()

# Total runs scored team for match

runs_per_innings = df_T20_ball.groupby(['Match_ID', 'batting_team'])['total_runs'].sum().reset_index()

plt.figure(figsize=(16,6))
sns.boxplot(x='batting_team', y='total_runs', data=runs_per_innings)
plt.title("Distribution of Total Runs per Match by Batting Team in T20's", color = 'red')
plt.xlabel("Batting Team",  color='blue', fontsize=12)
plt.ylabel("Runs", color='green', fontsize=12)
plt.xticks(rotation=90)
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/T20_Overall_Runs_ByTeam.png")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Calculate wickets per innings
wickets_per_innings = df_T20_ball.groupby(['Match_ID', 'team'])['is_wicket'].sum().reset_index()

# Step 2: Count number of matches each bowling team has played
matches_count = wickets_per_innings['team'].value_counts()

# Step 3: Keep only teams with at least 50 matches
teams_with_50plus = matches_count[matches_count >= 100].index

# Step 4: Filter data
filtered_wickets_per_innings = wickets_per_innings[wickets_per_innings['team'].isin(teams_with_50plus)]

# Step 5: Plot
plt.figure(figsize=(16,6))
sns.boxplot(x='team', y='is_wicket', data=filtered_wickets_per_innings)
plt.title("Distribution of Total Wickets Taken By Bowling Team Per Match in T20's", color='red')
plt.xlabel("Bowling Team", color='blue', fontsize=12)
plt.ylabel("Wickets", color='green', fontsize=12)
plt.xticks(rotation=90)

# Save the plot
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/T20_Overall_Wickets_ByTeam.png", dpi=300, bbox_inches='tight')

plt.show()

# Total Wickets taken by Bowling team per match

wickets_per_innings = df_T20_ball.groupby(['Match_ID', 'team'])['is_wicket'].sum().reset_index()

plt.figure(figsize=(16,6))
sns.boxplot(x='team', y='is_wicket', data=wickets_per_innings)
plt.title("Distribution of Total Wickets Taken By Bowling Team Per Match in T20's", color = 'red')
plt.xlabel("Bowling Team", color='blue', fontsize=12)
plt.ylabel("Wickets", color='green', fontsize=12)
plt.xticks(rotation=90)
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/T20_Overall_Wickets_ByTeam.png")
plt.show()

# Replace missing winners with 'No Result'
df_T20_info['outcome.winner'] = df_T20_info['outcome.winner'].fillna('No Result')

# Count number of wins per team
winner_counts = df_T20_info['outcome.winner'].value_counts().reset_index()
winner_counts.columns = ['Winner', 'Count']

# Step 1: Filter out 'No Result'
winner_counts_filtered = winner_counts[winner_counts['Winner'] != 'No Result']

# Step 2: Keep only teams with at least 50 wins
winner_counts_filtered = winner_counts_filtered[winner_counts_filtered['Count'] >= 50]

# Step 3: Plot
plt.figure(figsize=(16,6))
ax = sns.barplot(data=winner_counts_filtered, x='Winner', y='Count')
plt.title("Number of Matches Won by Each Team (Teams with ≥ 50 Wins)", color='red')
plt.xlabel("Winning Team", color='blue', fontsize=12)
plt.ylabel("Number of Matches", color='green', fontsize=12)
plt.xticks(rotation=75)
plt.tight_layout()

# Save the plot
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/T20_Most_Wins_ByTeam.png", dpi=300, bbox_inches='tight')

plt.show()

# Number of matches won by each team

df_T20_info['outcome.winner'] = df_T20_info['outcome.winner'].fillna('No Result')
plt.figure(figsize=(16,6))
winner_counts = df_T20_info['outcome.winner'].value_counts().reset_index()
winner_counts.columns = ['Winner', 'Count']
ax = sns.barplot(data=winner_counts, x='Winner', y='Count')
plt.title("Number of Matches Won by Each Team", color = 'red')
plt.xlabel("Winning Team", color='blue', fontsize=12)
plt.ylabel("Number of Matches", color='green', fontsize=12)
plt.xticks(rotation=75)
plt.tight_layout()
plt.savefig("/content/drive/MyDrive/Cricket_Project/Visualizations/T20_Most_Wins_ByTeam.png")
plt.show()

"""<h2><b><font color="gold">Batting Metrics of a Batter</font></b></h2>"""

# Basic batting metrics by batter

T20_batting_stats = (
    df_T20_ball.groupby('batter').agg(
        runs_scored = ('batsman_runs', 'sum'),
        balls_faced = ('batsman_runs', 'count'),
        fours = ('batsman_runs', lambda x: (x == 4).sum()),
        sixes = ('batsman_runs', lambda x: (x == 6).sum()),
        dismissals = ('is_wicket', 'sum')
    ).reset_index()
)

T20_innings_scores = df_T20_ball.groupby(['batter', 'Match_ID'])['batsman_runs'].sum().reset_index()
advanced = (
    T20_innings_scores.groupby('batter').agg(
        high_score = ('batsman_runs', 'max'),
        fifties = ('batsman_runs', lambda x: ((x >= 50) & (x < 100)).sum()),
        hundreds = ('batsman_runs', lambda x: ((x >= 100) & (x < 200)).sum()),
        double_hundreds = ('batsman_runs', lambda x: (x >= 200).sum()),
        score_std = ('batsman_runs', 'std')
    ).reset_index()
)

# Round off score_std with 2 decimal values
advanced['score_std'] = advanced['score_std'].round(2)

# Merging Basic Metrics and Adavance Metrics

T20_batting_stats = T20_batting_stats.merge(advanced, on='batter', how='left')

# Post-processing adding strike rate, batting average to the dataframe

T20_batting_stats['strike_rate'] = (T20_batting_stats['runs_scored'] / T20_batting_stats['balls_faced'] * 100).replace([np.inf, np.nan], 0).round(2)
T20_batting_stats['batting_average'] = (T20_batting_stats['runs_scored'] / T20_batting_stats['dismissals']).replace([np.inf, np.nan], 0).round(2)

# Finding Recent Form from last 5 matches
N = 5
T20_innings_scores['recent_form_runs'] = (
    T20_innings_scores.groupby('batter')['batsman_runs']
    .transform(lambda x: x.shift(1).rolling(window=N, min_periods=1).mean())
)
T20_innings_scores['career_avg_runs'] = (
    T20_innings_scores.groupby('batter')['batsman_runs'].transform(lambda x: x.expanding().mean())
)

"""<h2><b><font color="gold">Metrics of Batter Vs Opponent</font></b></h2>"""

# Metrics of Batter vs Opponent

T20_batter_vs_opp = (
    df_T20_ball.groupby(['batter', 'opposition']).agg(
        innings_played=('Match_ID', pd.Series.nunique),
        runs_scored=('batsman_runs', 'sum'),
        balls_faced=('batsman_runs', 'count'),
        dismissals=('is_wicket', 'sum')
    ).reset_index()
)
T20_batter_vs_opp['batting_average'] = (T20_batter_vs_opp['runs_scored'] /T20_batter_vs_opp['dismissals']).replace([np.inf, np.nan], 0).round(2)
T20_batter_vs_opp['strike_rate'] = (T20_batter_vs_opp['runs_scored'] / T20_batter_vs_opp['balls_faced'] * 100).replace([np.inf, np.nan], 0).round(2)

# Adding Boundary Percentage & Balls per Boundary to the dataframe

T20_batting_stats['boundary_pct'] = (
    (T20_batting_stats['fours'] + T20_batting_stats['sixes']) / T20_batting_stats['balls_faced'] * 100
).replace([np.inf, np.nan], 0).round(2)

T20_batting_stats['balls_per_boundary'] = (
    T20_batting_stats['balls_faced'] / (T20_batting_stats['fours'] + T20_batting_stats['sixes'])
).replace([np.inf, np.nan], 0).round(2)

# Wide Format Batter Overall Average Metrics and Average Vs Opponent

T20_batter_vs_opp_pivot = (
    T20_batter_vs_opp.pivot(index='batter', columns='opposition', values='batting_average')
    .add_prefix('avg_vs_')
    .reset_index()
)
T20_batting_summary = T20_batting_stats.merge(T20_batter_vs_opp_pivot, on='batter', how='left')

"""<h2><b><font color="gold">Metrics of Batter Vs venue</font></b></h2>"""

# Runs for each batter at each venue

T20_innings_scores = (
    df_T20_ball.groupby(['batter', 'venue', 'Match_ID'])['batsman_runs']
    .sum()
    .reset_index()
)

# Counting 50s and 100s at each venue for each batter

T20_fifty_hundred_stats = (
    T20_innings_scores.groupby(['batter', 'venue'])
    .agg(
        fifties=('batsman_runs', lambda x: ((x >= 50) & (x < 100)).sum()),
        hundreds=('batsman_runs', lambda x: ((x >= 100) & (x < 200)).sum()),
        double_hundreds=('batsman_runs', lambda x: (x >= 200).sum())
    )
    .reset_index()
)

# Core Batting Stats by batter and venue

T20_batter_vs_venue = (
    df_T20_ball.groupby(['batter', 'venue']).agg(
        innings_played=('Match_ID', pd.Series.nunique),
        runs_scored=('batsman_runs', 'sum'),
        balls_faced=('batsman_runs', 'count'),
        dismissals=('is_wicket', 'sum'),
        fours=('batsman_runs', lambda x: (x == 4).sum()),
        sixes=('batsman_runs', lambda x: (x == 6).sum())
    ).reset_index()
)

# Adding 50s/100s/200s to the venue stats df

T20_batter_vs_venue = T20_batter_vs_venue.merge(
    T20_fifty_hundred_stats, on=['batter', 'venue'], how='left'
)

# Advanced Batting Metrics

# Finding Batting Average for each venue

T20_batter_vs_venue['batting_average'] = (
    T20_batter_vs_venue['runs_scored'] / T20_batter_vs_venue['dismissals']
).replace([np.inf, np.nan], 0).round(2)

# Finding Strike Rate for each venue

T20_batter_vs_venue['strike_rate'] = (
    T20_batter_vs_venue['runs_scored'] / T20_batter_vs_venue['balls_faced'] * 100
).replace([np.inf, np.nan], 0).round(2)

# Filling NaN for fifties/hundreds/double_hundreds ----
T20_batter_vs_venue[['fifties', 'hundreds', 'double_hundreds']] = T20_batter_vs_venue[[
    'fifties', 'hundreds', 'double_hundreds'
]].fillna(0).astype(int)

# T20_batting_stats.to_excel("/content/drive/MyDrive/Cricket_Project/Stats_DF/T20_batting_stats.xlsx", index=False)
# T20_batter_vs_opp.to_excel("/content/drive/MyDrive/Cricket_Project/Stats_DF/T20_batter_vs_opp.xlsx", index=False)
# T20_batter_vs_venue.to_excel("/content/drive/MyDrive/Cricket_Project/Stats_DF/T20_batter_vs_venue.xlsx", index=False)
# T20_batting_summary.to_excel("/content/drive/MyDrive/Cricket_Project/Stats_DF/T20_batting_summary.xlsx", index=False)
# T20_innings_scores.to_excel("/content/drive/MyDrive/Cricket_Project/Stats_DF/T20_batting_innings_scores.xlsx", index=False)

"""<h2><b><font color="gold">Bowling Metrics for Bowler</font></b></h2>"""

# A maiden = an over with total_runs == 0 for a bowler in a match

T20_overs_summary = (
    df_T20_ball.groupby(['bowler', 'Match_ID', 'batting_team', 'over'])
    .agg(total_runs_in_over=('total_runs', 'sum'))
    .reset_index()
)
T20_overs_summary['is_maiden'] = (T20_overs_summary['total_runs_in_over'] == 0).astype(int)

T20_maidens_by_bowler = (
    T20_overs_summary.groupby('bowler')['is_maiden']
    .sum()
    .reset_index()
    .rename(columns={'is_maiden': 'maidens'})
)

# Bowling Metrics for bowler

T20_agg_dict = {
    'balls_bowled': ('bowler', 'count'),
    'runs_conceded': ('total_runs', 'sum'),
    'wickets': ('is_wicket', 'sum'),
    'fours_conceded': ('batsman_runs', lambda x: (x == 4).sum()),
    'sixes_conceded': ('batsman_runs', lambda x: (x == 6).sum()),
}

T20_bowling_stats = df_T20_ball.groupby('bowler').agg(**T20_agg_dict).reset_index()
T20_bowling_stats = T20_bowling_stats.merge(T20_maidens_by_bowler, on='bowler', how='left')
T20_bowling_stats['maidens'] = T20_bowling_stats['maidens'].fillna(0).astype(int)

# Per-Innings Bowling Summary

T20_innings_bowling = (
    df_T20_ball.groupby(['bowler', 'Match_ID', 'batting_team'])
    .agg(
        runs_conceded=('total_runs', 'sum'),
        wickets=('is_wicket', 'sum'),
        balls_bowled=('bowler', 'count')
    ).reset_index()
)

# Best innings figures (most wickets and then fewest runs)

T20_best_innings = (
    T20_innings_bowling.loc[
        T20_innings_bowling.groupby('bowler')['wickets'].idxmax()
    ][['bowler', 'wickets', 'runs_conceded']]
    .rename(columns={'wickets': 'best_innings_wickets', 'runs_conceded': 'best_innings_runs_conceded'})
)

# 5WI, std dev
advanced_bowl = (
    T20_innings_bowling.groupby('bowler').agg(
        five_wicket_hauls=('wickets', lambda x: (x >= 5).sum()),
        wickets_std=('wickets', 'std')
    ).reset_index()
)
advanced_bowl['wickets_std'] = advanced_bowl['wickets_std'].replace([np.inf, np.nan], 0).round(2)

# Merging advanced metrics

advanced_bowl = advanced_bowl.merge(T20_best_innings, on='bowler', how='left')

# Merging Basic and Advanced metrics

T20_bowling_stats = T20_bowling_stats.merge(advanced_bowl, on='bowler', how='left')
T20_bowling_stats['best_innings_wickets'] = T20_bowling_stats['best_innings_wickets'].fillna(0).astype(int)

# Post-Processing Metrics: Bowling Average, Strike Rate, Economy

# Finding Bowling Average

T20_bowling_stats['bowling_average'] = (
    T20_bowling_stats['runs_conceded'] / T20_bowling_stats['wickets']
).replace([np.inf, np.nan], 0).round(2)


# Finding Bowler Strike rate

T20_bowling_stats['strike_rate'] = (
    T20_bowling_stats['balls_bowled'] / T20_bowling_stats['wickets']
).replace([np.inf, np.nan], 0).round(2)


# Finding Bowler Economy

T20_bowling_stats['economy'] = (
    T20_bowling_stats['runs_conceded'] / T20_bowling_stats['balls_bowled'] * 6
).replace([np.inf, np.nan], 0).round(2)


# Finding Wicket per balls

T20_bowling_stats['balls_per_wicket'] = (
    T20_bowling_stats['balls_bowled'] / T20_bowling_stats['wickets']
).replace([np.inf, np.nan], 0).round(2)


#Finding Boundaary Percentage

T20_bowling_stats['boundary_pct'] = (
    (T20_bowling_stats['fours_conceded'] + T20_bowling_stats['sixes_conceded']) /
    T20_bowling_stats['balls_bowled'] * 100
).replace([np.inf, np.nan], 0).round(2)

"""<h2><b><font color="gold">Bowler Vs Opponent Analysis</font></b></h2>"""

# Bowler vs Opposition Analysis

T20_bowler_vs_opp = (
    df_T20_ball.groupby(['bowler', 'opposition']).agg(
        matches=('Match_ID', pd.Series.nunique),
        balls_bowled=('bowler', 'count'),
        runs_conceded=('total_runs', 'sum'),
        wickets=('is_wicket', 'sum'),
    ).reset_index()
)

# Adding Bowling_Avg, Balls_Bowled, Economy fields to the dataframe

T20_bowler_vs_opp['bowling_average'] = (
    T20_bowler_vs_opp['runs_conceded'] / T20_bowler_vs_opp['wickets']
).replace([np.inf, np.nan], 0).round(2)
T20_bowler_vs_opp['strike_rate'] = (
    T20_bowler_vs_opp['balls_bowled'] / T20_bowler_vs_opp['wickets']
).replace([np.inf, np.nan], 0).round(2)
T20_bowler_vs_opp['economy'] = (
    T20_bowler_vs_opp['runs_conceded'] / T20_bowler_vs_opp['balls_bowled'] * 6
).replace([np.inf, np.nan], 0).round(2)

# Wide Format (Pivot for Averages by Opposition)
T20_bowler_vs_opp_pivot = (
    T20_bowler_vs_opp.pivot(index='bowler', columns='opposition', values='bowling_average')
    .add_prefix('avg_vs_')
    .reset_index()
)
T20_bowling_summary = T20_bowling_stats.merge(T20_bowler_vs_opp_pivot, on='bowler', how='left')

"""<h2><b><font color="gold"> Bowler Vs venue Analysis</font></b></h2>"""

# Bowler Vs venue

T20_bowler_vs_venue = (
    df_T20_ball.groupby(['bowler', 'venue']).agg(
        matches=('Match_ID', pd.Series.nunique),
        balls_bowled=('bowler', 'count'),
        runs_conceded=('total_runs', 'sum'),
        wickets=('is_wicket', 'sum'),
        fours_conceded=('batsman_runs', lambda x: (x == 4).sum()),
        sixes_conceded=('batsman_runs', lambda x: (x == 6).sum())
    ).reset_index()
)

# Adding Bowling_Avg, Strike rate, Economy fields to the dataframe

T20_bowler_vs_venue['bowling_average'] = (
    T20_bowler_vs_venue['runs_conceded'] / T20_bowler_vs_venue['wickets']
).replace([np.inf, np.nan], 0).round(2)
T20_bowler_vs_venue['strike_rate'] = (
    T20_bowler_vs_venue['balls_bowled'] / T20_bowler_vs_venue['wickets']
).replace([np.inf, np.nan], 0).round(2)
T20_bowler_vs_venue['economy'] = (
    T20_bowler_vs_venue['runs_conceded'] / T20_bowler_vs_venue['balls_bowled'] * 6
).replace([np.inf, np.nan], 0).round(2)

# Finding Recent Form from last 5 matches

N = 5
T20_innings_bowling['recent_form_wickets'] = (
    T20_innings_bowling.groupby('bowler')['wickets']
    .transform(lambda x: x.shift(1).rolling(window=N, min_periods=1).mean())
)
T20_innings_bowling['career_avg_wickets'] = (
    T20_innings_bowling.groupby('bowler')['wickets'].transform(lambda x: x.expanding().mean())
)

"""***Importing All The Required Libraries***

"""

# Importing all the required Libraries
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, r2_score

"""<h2><b><font color="gold"> Preparing Batting Data for Modeling</font></b></h2>

***Base rows (one per batter–match)***
"""

# Select match-level columns per batter
base_cols = [
    'Match_ID','batter','venue','opposition','season','city',
    'match_type','event.match_number','outcome.winner',
    'outcome.by.runs','outcome.by.wickets','outcome.result','player_id'
]
T20_bat_model_df = df_T20_ball[base_cols].drop_duplicates()

"""***Merge career stats***"""

# Add overall career batting stats per batter (suffix _career)
career = T20_batting_stats.copy()
career = career.rename(columns={c: f"{c}_career" for c in career.columns if c != 'batter'})
T20_bat_model_df = T20_bat_model_df.merge(career, on='batter', how='left')

"""***Merge innings scores***"""

# Add per-match runs (target) and simple form features
inn_cols = ['batter','Match_ID','batsman_runs','recent_form_runs','career_avg_runs']
inn_cols = [c for c in inn_cols if c in T20_innings_scores.columns]  # safe selection
inn = T20_innings_scores[inn_cols].copy()

T20_bat_model_df = T20_bat_model_df.merge(inn, on=['batter','Match_ID'], how='left')

"""***Merge batter vs opponent stats***"""

# Add each batter's record versus each opposition (suffix _vs_opp)
vs_opp = T20_batter_vs_opp.copy()
vs_opp = vs_opp.rename(columns={c: f"{c}_vs_opp" for c in vs_opp.columns if c not in ['batter','opposition']})
T20_bat_model_df = T20_bat_model_df.merge(vs_opp, on=['batter','opposition'], how='left')

"""***Merge batter vs venue stats***"""

# Add each batter's record at each venue (suffix _vs_venue)
vs_venue = T20_batter_vs_venue.copy()
vs_venue = vs_venue.rename(columns={c: f"{c}_vs_venue" for c in vs_venue.columns if c not in ['batter','venue']})
T20_bat_model_df = T20_bat_model_df.merge(vs_venue, on=['batter','venue'], how='left')

"""***Merge batting summary***"""

# Add overall summary metrics per batter (suffix _summary)
summary = T20_batting_summary.copy()
summary = summary.rename(columns={c: f"{c}_summary" for c in summary.columns if c != 'batter'})
T20_bat_model_df = T20_bat_model_df.merge(summary, on='batter', how='left')

"""***Venue–opposition aggregates***"""

# Compute extra stats per (batter, venue, opposition) from ball-by-ball and merge
tmp = df_T20_ball[['batter','venue','opposition','Match_ID','batsman_runs','ball_number','wicket.player_out']].copy()
tmp['is_batter_out'] = (tmp['wicket.player_out'] == tmp['batter']).astype(int)

venue_opp = tmp.groupby(['batter','venue','opposition']).agg(
    innings_played_venue_opp=('Match_ID','nunique'),
    runs_scored_venue_opp=('batsman_runs','sum'),
    balls_faced_venue_opp=('ball_number','count'),
    dismissals_venue_opp=('is_batter_out','sum')
).reset_index()

match_runs = tmp.groupby(['batter','venue','opposition','Match_ID'])['batsman_runs'].sum().reset_index()
f50  = match_runs.assign(is50 =(match_runs['batsman_runs']>=50 ).astype(int)) \
                 .groupby(['batter','venue','opposition'])['is50'].sum().reset_index(name='fifties_venue_opp')
f100 = match_runs.assign(is100=(match_runs['batsman_runs']>=100).astype(int)) \
                 .groupby(['batter','venue','opposition'])['is100'].sum().reset_index(name='hundreds_venue_opp')

venue_opp = venue_opp.merge(f50,  on=['batter','venue','opposition'], how='left')
venue_opp = venue_opp.merge(f100, on=['batter','venue','opposition'], how='left')

venue_opp['batting_average_venue_opp'] = np.where(
    venue_opp['dismissals_venue_opp'] > 0,
    venue_opp['runs_scored_venue_opp'] / venue_opp['dismissals_venue_opp'],
    venue_opp['runs_scored_venue_opp']
)

T20_bat_model_df = T20_bat_model_df.merge(venue_opp, on=['batter','venue','opposition'], how='left')

"""***Match order + recent runs form***"""

# Create per-batter match order and rolling recent form (runs) with windows 3, 5, 10
ord_df = df_T20_ball[['batter','Match_ID','dates','season','event.match_number']].drop_duplicates().copy()
ord_df['dates'] = pd.to_datetime(ord_df['dates'], errors='coerce')
ord_df = ord_df.sort_values(['batter','dates','season','event.match_number','Match_ID'])
ord_df['Match_ID_order'] = ord_df.groupby('batter').cumcount() + 1

T20_bat_model_df = T20_bat_model_df.merge(ord_df[['batter','Match_ID','Match_ID_order']], on=['batter','Match_ID'], how='left')
print("After match order:", T20_bat_model_df.shape)

T20_bat_model_df = T20_bat_model_df.sort_values(['batter','Match_ID_order']).copy()
for w in [3, 5, 10]:
    T20_bat_model_df[f'recent_runs_mean_{w}'] = (
        T20_bat_model_df.groupby('batter')['batsman_runs'].shift(1).rolling(w, min_periods=1).mean()
    )

"""***Clean missing values and encode categoricals***"""

# Fill NAs, define X/y, and label-encode object features for modeling
from sklearn.preprocessing import LabelEncoder

T20_bat_model_df = T20_bat_model_df.loc[:, ~T20_bat_model_df.columns.duplicated()].copy()

for c in T20_bat_model_df.select_dtypes(include='object').columns:
    T20_bat_model_df[c] = T20_bat_model_df[c].fillna('Unknown')
for c in T20_bat_model_df.select_dtypes(include=[np.number]).columns:
    T20_bat_model_df[c] = T20_bat_model_df[c].fillna(T20_bat_model_df[c].median())

y = T20_bat_model_df['batsman_runs']
drop_cols = ['batsman_runs','Match_ID','batter','player_id']
drop_cols = [c for c in drop_cols if c in T20_bat_model_df.columns]
X = T20_bat_model_df.drop(columns=drop_cols)

encoders = {}
# Encode categorical columns directly into X
for c in X.select_dtypes(include='object').columns:
    le = LabelEncoder()
    X[c] = le.fit_transform(X[c].astype(str))
    encoders[c] = le

print("After clean + encode:", X.shape)

import joblib

joblib.dump(encoders, "label_encoders_t20_batters.joblib")

maps = {c: {cls: int(i) for i, cls in enumerate(le.classes_)} for c, le in encoders.items()}
joblib.dump(maps, "label_maps_t20_batters.joblib")

T20_bat_model_df.to_csv("T20_bat_model_df.csv", index=False)

# Cross checking any object columns before model training
object_cols = list(X.select_dtypes(include=['object']).columns)
print("Columns to be label encoded:", object_cols)

"""***Splitting Data for Model Training***"""

# Spliting data with all the columns
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""<h2><b><font color="gold">Random Forest</font></b></h2>"""

# Training Random Forest Model
# This code trains a Random Forest model to predict runs, and prints out how well the model performed using MAE and R2 metrics

rf_T20_bat = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)
rf_T20_bat.fit(X_train, y_train)

# Predicting
y_pred = rf_T20_bat.predict(X_test)

# Round predicted values
y_pred_rounded = np.round(y_pred).astype(int)

# Evaluate on rounded predictions
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)

# Print results
print(f"Random Forest MAE (Rounded): {mae:.2f}")
print(f"Random Forest R2 (Rounded): {r2:.3f}")

# Comparing Actual Runs and Predicted Runs

comparison_df = pd.DataFrame({
    'Actual Runs': y_test.values,
    'Predicted Runs': np.round(y_pred).astype(int)
})

# Show the first 20 rows for a quick check
print(comparison_df.head(20))

# Plotting the top 20 feature importances as a bar chart from Random Forest Regressor

importances = rf_T20_bat.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(12,5))
plt.title("Feature Importance (Random Forest)")
plt.bar(range(20), importances[indices][:20])
plt.xticks(range(20), [X.columns[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold"> Linear Regression</font></b></h2>"""

# Training Linear Regression Model
#This code trains Linear Regression model to predict runs, and prints out how well the model performed using MAE and R2 metrics


lr_T20_bat = LinearRegression()
lr_T20_bat.fit(X_train, y_train)

# Predicting
y_pred = lr_T20_bat.predict(X_test)

# Round predicted values
y_pred_rounded = np.round(y_pred).astype(int)

# Evaluate on rounded predictions
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)

# Print results
print(f"Linear Regression MAE (Rounded): {mae:.2f}")
print(f"Linear Regression R2 (Rounded): {r2:.3f}")

# Comparing Actual Runs and Predicted Runs

comparison_df = pd.DataFrame({
    'Actual Runs': y_test.values,
    'Predicted Runs': np.round(y_pred).astype(int)
})

# Show the first 20 rows for a quick check
print(comparison_df.head(20))

# Get absolute coefficients as feature importance
coefs = np.abs(lr_T20_bat.coef_)
indices = np.argsort(coefs)[::-1]

# Plot the top 20 features
plt.figure(figsize=(12, 5))
plt.title("Feature Importance (Linear Regression)")
plt.bar(range(20), coefs[indices][:20])
plt.xticks(range(20), [X.columns[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">XGBoost</font></b></h2>"""

# Training XGBoost Model
#This code trains a XGBoost model to predict runs, and prints out how well the model performed using MAE and R2 metrics


import xgboost as xgb

xgb_reg_T20_bat = xgb.XGBRegressor(
    n_estimators=100,       # Number of trees
    learning_rate=0.1,      # Step size shrinkage
    max_depth=6,            # Depth of trees
    subsample=0.8,          # Row sampling
    colsample_bytree=0.8,   # Feature sampling
    random_state=42,
    n_jobs=-1               # Use all CPU cores
)

xgb_reg_T20_bat.fit(X_train, y_train)

# Predict using XGBoost
y_pred = xgb_reg_T20_bat.predict(X_test)

# Round predicted values
y_pred_rounded = np.round(y_pred).astype(int)

# Evaluate with rounded predictions
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)

# Print results
print(f"XGBoost MAE (Rounded): {mae:.2f}")
print(f"XGBoost R2 (Rounded): {r2:.3f}")

# Comparing Actual Runs and Predicted Runs

comparison_df = pd.DataFrame({
    'Actual Runs': y_test.values,
    'Predicted Runs': np.round(y_pred).astype(int)
})

# Show the first 20 rows for a quick check
print(comparison_df.head(20))

# Get feature importances from the XGBoost model
importances = xgb_reg_T20_bat.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot the top 20 features
plt.figure(figsize=(12, 5))
plt.title("Feature Importance (XGBoost)")
plt.bar(range(20), importances[indices][:20])
plt.xticks(range(20), [X.columns[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">LightGBM</font></b></h2>"""

# Training LightGBM Model
#This code trains a LightGBM model to predict runs, and prints out how well the model performed using MAE and R2 metrics

import lightgbm as lgb

lgb_reg_T20_bat = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=7,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    verbose=-1,
    n_jobs=-1
)

lgb_reg_T20_bat.fit(X_train, y_train)

# Predict using LightGBM
y_pred = lgb_reg_T20_bat.predict(X_test)

# Round predicted values
y_pred_rounded = np.round(y_pred).astype(int)

# Evaluate with rounded predictions
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)

# Print results
print(f"LightGBM MAE (Rounded): {mae:.2f}")
print(f"LightGBM R2 (Rounded): {r2:.3f}")

joblib.dump(lgb_reg_T20_bat, "lgb_reg_t20_bat.joblib")

# Comparing Actual Runs and Predicted Runs

comparison_df = pd.DataFrame({
    'Actual Runs': y_test.values,
    'Predicted Runs': np.round(y_pred).astype(int)
})

# Show the first 20 rows for a quick check
print(comparison_df.head(20))

# Get feature importances
importances = lgb_reg_T20_bat.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot the top 20 important features
plt.figure(figsize=(12, 5))
plt.title("Feature Importance (LightGBM)")
plt.bar(range(20), importances[indices][:20])
plt.xticks(range(20), [X.columns[i] for i in indices[:20]], rotation=90)
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">Standardization</font></b></h2>"""

# Fix column names before scaling
X.columns = X.columns.str.replace(' ', '_')

# Standardizing features and preserving column names
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Train-test split
X_train_std, X_test_std, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Training and evaluating models

# Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_std, y_train)
y_pred_rf = np.round(rf.predict(X_test_std)).astype(int)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest (Rounded): MAE: {mae_rf:.3f} R2: {r2_rf:.3f}")

# Linear Regression
lr = LinearRegression()
lr.fit(X_train_std, y_train)
y_pred_lr = np.round(lr.predict(X_test_std)).astype(int)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)
print(f"Linear Regression (Rounded): MAE: {mae_lr:.3f} R2: {r2_lr:.3f}")

# XGBoost
xgb_reg = XGBRegressor(random_state=42, verbosity=0)
xgb_reg.fit(X_train_std, y_train)
y_pred_xgb = np.round(xgb_reg.predict(X_test_std)).astype(int)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost (Rounded): MAE: {mae_xgb:.3f} R2: {r2_xgb:.3f}")

# LightGBM
lgb_reg = LGBMRegressor(random_state=42)
lgb_reg.fit(X_train_std, y_train)
y_pred_lgb = np.round(lgb_reg.predict(X_test_std)).astype(int)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)
print(f"LightGBM (Rounded): MAE: {mae_lgb:.3f} R2: {r2_lgb:.3f}")

"""<h2><b><font color="gold">Normalization</font></b></h2>"""

# Normalizing and preserving feature names
scaler_norm = MinMaxScaler()
X_norm = pd.DataFrame(scaler_norm.fit_transform(X), columns=X.columns)

# Train-test split
X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X_norm, y, test_size=0.2, random_state=42)

# Training and evaluating models

#  Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_norm, y_train_norm)
y_pred_rf = np.round(rf.predict(X_test_norm)).astype(int)
mae_rf = mean_absolute_error(y_test_norm, y_pred_rf)
r2_rf = r2_score(y_test_norm, y_pred_rf)
print(f"Random Forest (Rounded): MAE: {mae_rf:.3f} R2: {r2_rf:.3f}")

#  Linear Regression
lr = LinearRegression()
lr.fit(X_train_norm, y_train_norm)
y_pred_lr = np.round(lr.predict(X_test_norm)).astype(int)
mae_lr = mean_absolute_error(y_test_norm, y_pred_lr)
r2_lr = r2_score(y_test_norm, y_pred_lr)
print(f"Linear Regression (Rounded): MAE: {mae_lr:.3f} R2: {r2_lr:.3f}")

#  XGBoost
xgb_reg = XGBRegressor(random_state=42, verbosity=0)
xgb_reg.fit(X_train_norm, y_train_norm)
y_pred_xgb = np.round(xgb_reg.predict(X_test_norm)).astype(int)
mae_xgb = mean_absolute_error(y_test_norm, y_pred_xgb)
r2_xgb = r2_score(y_test_norm, y_pred_xgb)
print(f"XGBoost (Rounded): MAE: {mae_xgb:.3f} R2: {r2_xgb:.3f}")

#  LightGBM
lgb_reg = LGBMRegressor(random_state=42)
lgb_reg.fit(X_train_norm, y_train_norm)
y_pred_lgb = np.round(lgb_reg.predict(X_test_norm)).astype(int)
mae_lgb = mean_absolute_error(y_test_norm, y_pred_lgb)
r2_lgb = r2_score(y_test_norm, y_pred_lgb)
print(f"LightGBM (Rounded): MAE: {mae_lgb:.3f} R2: {r2_lgb:.3f}")

"""<h2><b><font color="gold">LightGBM Hyperparameter Tuning</font></b></h2>


"""

# Defining the parameter grid
param_dist = {
    'num_leaves': [15, 31, 63, 127],
    'max_depth': [-1, 5, 10, 15],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 300, 500],
    'min_child_samples': [5, 10, 20, 30],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

X.columns = X.columns.str.replace(' ', '_')

# Initialize model
lgbm = LGBMRegressor(random_state=42, verbose=-1)

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=lgbm,
    param_distributions=param_dist,
    n_iter=30,  # reduce if very slow
    scoring='neg_mean_absolute_error',
    cv=3,
    n_jobs=-1
)

# Fit the random search
random_search.fit(X_train, y_train)

# Predicting and evaluating
best_lgbm = random_search.best_estimator_
y_pred = np.round(best_lgbm.predict(X_test)).astype(int)

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Best Parameters:", random_search.best_params_)
print(f"Tuned LightGBM (Rounded): MAE: {mae:.3f}, R2: {r2:.3f}")

results = pd.DataFrame({
    'Actual': y_test.values.astype(int),
    'Predicted (rounded)': y_pred.astype(int)
})
results['Error'] = results['Predicted (rounded)'] - results['Actual']

print("Actual vs Predicted (first 20 rows):")
print(results.head(20))

# Use your tuned model from RandomizedSearchCV
importances_lgb = pd.Series(best_lgbm.feature_importances_, index=X.columns)
importances_lgb_sorted = importances_lgb.sort_values(ascending=False).head(20)

plt.figure(figsize=(10, 6))
importances_lgb_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (Tuned LightGBM)")
plt.gca().invert_yaxis()
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">SHAP explanations for tuned LightGBM </font></b></h2>"""

import shap

# Use a small sample of X_test for speed (uses all if smaller than 1000)
X_explain = X_test.sample(n=min(1000, len(X_test)), random_state=42)

# Build explainer and compute SHAP values
explainer = shap.TreeExplainer(best_lgbm)
shap_values = explainer.shap_values(X_explain)

"""***SHAP Beeswarm Plot***"""

# SHAP beeswarm showing impact of each feature on individual predictions
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,
    X_explain,
    max_display=10,
    show=False
)
plt.title("SHAP value (impact on model output)")
plt.tight_layout()
plt.show()

"""***SHAP Bar Plot***

"""

# SHAP bar chart showing average absolute impact of each feature
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,
    X_explain,
    plot_type="bar",
    max_display=10,
    show=False
)
plt.title("mean(|SHAP value|) – average impact on model output")
plt.xlabel("mean(|SHAP value|) (average impact on model output)")
plt.tight_layout()
plt.show()

"""***SHAP Single-Feature Dependence Plot***"""

# SHAP dependence plot showing how one feature’s values affect predictions

# Pick the top feature by mean absolute SHAP value
mean_abs = np.abs(shap_values).mean(axis=0)
top_idx = np.argsort(mean_abs)[-1]
top_feat = X_explain.columns[top_idx]   # e.g., "vs_venue_wickets"

plt.figure(figsize=(10, 6))
shap.dependence_plot(
    top_feat,          # feature to visualize
    shap_values,       # SHAP values from explainer
    X_explain,         # data sample used for explanations
    show=False
)
plt.title(f"SHAP Dependence Plot – {top_feat}")
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">Preparing Data For Bowler Model Training</font></b></h2>

***Copy base dataframe & fill extras***
"""

# Make a working copy
t20_df = df_T20_ball.copy()

# Fill missing extras with 0
t20_df['extras.wides']   = t20_df['extras.wides'].fillna(0)
t20_df['extras.noballs'] = t20_df['extras.noballs'].fillna(0)
t20_df['extras.byes']    = t20_df['extras.byes'].fillna(0)
t20_df['extras.legbyes'] = t20_df['extras.legbyes'].fillna(0)

"""***Create per-ball helper columns***"""

# Legal delivery: not a wide and not a no-ball
t20_df['is_legal_ball'] = ((t20_df['extras.wides'] == 0) &
                           (t20_df['extras.noballs'] == 0)).astype(int)

# Runs conceded per ball = batsman runs + wides + no-balls
t20_df['runs_conceded_ball'] = (
    t20_df['batsman_runs'].fillna(0) +
    t20_df['extras.wides'] +
    t20_df['extras.noballs']
)

# Bowler extras (only wides + no-balls)
t20_df['bowler_extras_ball'] = t20_df['extras.wides'] + t20_df['extras.noballs']

"""***Calculate runs conceded per over***"""

# Total runs conceded in each over for each bowler in each match
T20_over_runs = (
    t20_df.groupby(['Match_ID', 'bowler', 'over'])['runs_conceded_ball']
          .sum()
          .reset_index(name='runs_in_over')
)

"""***Identify maiden overs per bowler per match***"""

# Mark maiden overs (0 runs conceded in an over)
T20_over_runs['is_maiden_over'] = (T20_over_runs['runs_in_over'] == 0).astype(int)

# Count maiden overs for each bowler in each match
T20_maidens = (
    T20_over_runs.groupby(['Match_ID', 'bowler'])['is_maiden_over']
                 .sum()
                 .reset_index(name='maidens_in_match')
)

"""***Build match-level aggregates + add opposition & metadata***"""

# Totals per (Match_ID, bowler)
T20_agg = (
    t20_df.groupby(['Match_ID', 'bowler'])
          .agg({
              'is_legal_ball': 'sum',
              'runs_conceded_ball': 'sum',
              'is_wicket': 'sum',
              'bowler_extras_ball': 'sum'
          })
          .reset_index()
          .rename(columns={
              'is_legal_ball': 'balls_bowled_in_match',
              'runs_conceded_ball': 'runs_conceded_in_match',
              'is_wicket': 'wickets_in_match',
              'bowler_extras_ball': 'extras_in_match'
          })
)

# Economy (runs per over) and Strike Rate (balls per wicket)
T20_agg['econ_rate_in_match']   = T20_agg['runs_conceded_in_match'] / (T20_agg['balls_bowled_in_match'] / 6)
T20_agg['strike_rate_in_match'] = T20_agg['balls_bowled_in_match'] / T20_agg['wickets_in_match'].replace(0, np.nan)

# Opposition = batting_team faced most balls in that match
T20_opp = (
    t20_df.groupby(['Match_ID', 'bowler', 'batting_team'])
          .size()
          .reset_index(name='balls_against_team')
    .sort_values(['Match_ID','bowler','balls_against_team'], ascending=[True, True, False])
    .drop_duplicates(['Match_ID','bowler'])
    .rename(columns={'batting_team': 'opposition'})[['Match_ID','bowler','opposition']]
)

# Match metadata (first row per match)
T20_meta = (
    t20_df.groupby('Match_ID')[[
        'venue','season','match_type','outcome.winner','outcome.by.runs',
        'outcome.by.wickets','outcome.result','dates'
    ]].first().reset_index()
    .rename(columns={'dates': 'match_date'})
)

# Combine all parts
T20_bowl_model_df = (
    T20_agg.merge(T20_maidens, on=['Match_ID','bowler'], how='left')
           .merge(T20_opp,     on=['Match_ID','bowler'], how='left')
           .merge(T20_meta,    on='Match_ID',            how='left')
)

# Clean wickets to int
T20_bowl_model_df['wickets_in_match'] = T20_bowl_model_df['wickets_in_match'].fillna(0).astype(int)

"""***Merge career bowling stats***"""

# Keep relevant columns and prefix with career_
T20_career_cols = ['bowler','balls_bowled','runs_conceded','wickets','fours_conceded',
                   'sixes_conceded','maidens','five_wicket_hauls','wickets_std',
                   'best_innings_wickets','best_innings_runs_conceded',
                   'bowling_average','strike_rate','economy','balls_per_wicket','boundary_pct']

T20_career_df = T20_bowling_stats[T20_career_cols].rename(
    columns={c: f'career_{c}' for c in T20_career_cols if c != 'bowler'}
)

T20_bowl_model_df = T20_bowl_model_df.merge(T20_career_df, on='bowler', how='left')

"""***Merge opposition-level stats***"""

# Select and rename vs-opp columns
T20_opp_df = T20_bowler_vs_opp[[
    'bowler','opposition','matches','balls_bowled','runs_conceded','wickets',
    'bowling_average','strike_rate','economy'
]].rename(columns={
    'matches':'vs_opp_matches',
    'balls_bowled':'vs_opp_balls_bowled',
    'runs_conceded':'vs_opp_runs_conceded',
    'wickets':'vs_opp_wickets',
    'bowling_average':'vs_opp_bowling_average',
    'strike_rate':'vs_opp_strike_rate',
    'economy':'vs_opp_economy'
})
# Merge on (Match_ID, bowler)'s opposition
T20_bowl_model_df = T20_bowl_model_df.merge(T20_opp_df, on=['bowler','opposition'], how='left')

"""***Merge venue-level stats***"""

# Select and rename vs-venue columns
T20_venue_df = T20_bowler_vs_venue[[
    'bowler','venue','matches','balls_bowled','runs_conceded','wickets',
    'fours_conceded','sixes_conceded','bowling_average','strike_rate','economy'
]].rename(columns={
    'matches':'vs_venue_matches',
    'balls_bowled':'vs_venue_balls_bowled',
    'runs_conceded':'vs_venue_runs_conceded',
    'wickets':'vs_venue_wickets',
    'fours_conceded':'vs_venue_fours_conceded',
    'sixes_conceded':'vs_venue_sixes_conceded',
    'bowling_average':'vs_venue_bowling_average',
    'strike_rate':'vs_venue_strike_rate',
    'economy':'vs_venue_economy'
})
# Merge on (bowler, venue)
T20_bowl_model_df = T20_bowl_model_df.merge(T20_venue_df, on=['bowler','venue'], how='left')

"""***Merge innings-level stats***"""

# Sum innings to match level and merge
T20_ib_match = (
    T20_innings_bowling
    .groupby(['Match_ID','bowler'], as_index=False)
    .agg({
        'runs_conceded':'sum',
        'wickets':'sum',
        'balls_bowled':'sum',
        'recent_form_wickets':'first',
        'career_avg_wickets':'first',
        'batting_team':'first'
    })
    .rename(columns={
        'runs_conceded':'innings_runs_conceded',
        'wickets':'innings_wickets',
        'balls_bowled':'innings_balls_bowled',
        'batting_team':'innings_batting_team'
    })
)

T20_bowl_model_df = T20_bowl_model_df.merge(T20_ib_match, on=['Match_ID','bowler'], how='left')

"""***Merge opponent averages***"""

# Pick all average-vs-opponent columns and merge by bowler
T20_opp_avg_cols = [c for c in T20_bowling_summary.columns if c.startswith('avg_vs_')]
T20_bowler_opp_avg = T20_bowling_summary[['bowler'] + T20_opp_avg_cols].copy()

T20_bowl_model_df = T20_bowl_model_df.merge(T20_bowler_opp_avg, on='bowler', how='left')

"""***Create recent-form rolling averages***"""

# Create rolling means from previous matches (grouped by bowler)
# Uses columns innings_wickets, innings_runs_conceded
windows = [3, 5, 10, 15, 20]

# Ensure base columns exist
T20_bowl_model_df['innings_wickets'] = T20_bowl_model_df['innings_wickets'].fillna(0)
T20_bowl_model_df['innings_runs_conceded'] = T20_bowl_model_df['innings_runs_conceded'].fillna(0)

# Rolling means for wickets
for w in windows:
    col = f'recent_wickets_mean_{w}'
    T20_bowl_model_df[col] = (
        T20_bowl_model_df.groupby('bowler')['innings_wickets']
                         .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())
    )

# Rolling means for runs conceded
for w in windows:
    col = f'recent_runs_conceded_mean_{w}'
    T20_bowl_model_df[col] = (
        T20_bowl_model_df.groupby('bowler')['innings_runs_conceded']
                         .transform(lambda s: s.shift(1).rolling(w, min_periods=1).mean())
    )

"""***Create ratio features***"""

# Venue vs Career
T20_bowl_model_df['venue_wickets_ratio'] = (
    T20_bowl_model_df['vs_venue_wickets'] / T20_bowl_model_df['career_avg_wickets'].replace(0, np.nan)
)
T20_bowl_model_df['venue_sr_ratio'] = (
    T20_bowl_model_df['vs_venue_strike_rate'] / T20_bowl_model_df['career_strike_rate'].replace(0, np.nan)
)

# Opposition vs Career
T20_bowl_model_df['opp_wickets_ratio'] = (
    T20_bowl_model_df['vs_opp_wickets'] / T20_bowl_model_df['career_avg_wickets'].replace(0, np.nan)
)
T20_bowl_model_df['opp_sr_ratio'] = (
    T20_bowl_model_df['vs_opp_strike_rate'] / T20_bowl_model_df['career_strike_rate'].replace(0, np.nan)
)

# Clean infinities
for c in ['venue_wickets_ratio','venue_sr_ratio','opp_wickets_ratio','opp_sr_ratio']:
    T20_bowl_model_df[c] = T20_bowl_model_df[c].replace([np.inf, -np.inf], np.nan)

"""***Final Clean and Label Encoding***"""

# Remove duplicate columns (if any)
T20_bowl_model_df = T20_bowl_model_df.loc[:, ~T20_bowl_model_df.columns.duplicated()]

# Fill missing values
for c in T20_bowl_model_df.select_dtypes(include='object').columns:
    T20_bowl_model_df[c] = T20_bowl_model_df[c].fillna('Unknown')
for c in T20_bowl_model_df.select_dtypes(include=[np.number]).columns:
    T20_bowl_model_df[c] = T20_bowl_model_df[c].fillna(T20_bowl_model_df[c].median())

# Define target
y = T20_bowl_model_df['wickets_in_match']

# Final drop list (leakage-safe: target, IDs, post-match aggregates, innings stats, outcomes)
drop_cols = [
    # target & IDs
    'wickets_in_match','Match_ID','player_id','bowler','match_date',
    # post-match aggregates (leakage)
    'balls_bowled_in_match','runs_conceded_in_match','extras_in_match',
    'maidens_in_match','econ_rate_in_match','strike_rate_in_match',
    # innings stats (leakage)
    'innings_runs_conceded','innings_wickets','innings_balls_bowled','innings_batting_team',
    # match outcomes (leakage)
    'outcome.winner','outcome.by.runs','outcome.by.wickets','outcome.result','outcome.summary'
]
drop_cols = [c for c in drop_cols if c in T20_bowl_model_df.columns]

# Build features
X = T20_bowl_model_df.drop(columns=drop_cols)

encoders = {}
# Encode categorical columns directly into X
for c in X.select_dtypes(include='object').columns:
    le = LabelEncoder()
    X[c] = le.fit_transform(X[c].astype(str))
    encoders[c] = le

# Final check
print("X shape:", X.shape)
print("Total nulls in X:", int(X.isnull().sum().sum()))

joblib.dump(encoders, "label_encoders_t20_bowl.joblib")

T20_bowl_model_df.to_csv("T20_bowl_model_df.csv", index=False)

maps = {c: {cls: int(i) for i, cls in enumerate(le.classes_)} for c, le in encoders.items()}
joblib.dump(maps, "label_maps_t20_bowl.joblib")

# Cross checking any object columns before model training
object_cols = list(X.select_dtypes(include=['object']).columns)
print("Columns to be label encoded:", object_cols)

"""***Splitting The Data***"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""<h2><b><font color="gold">Random Forest</font></b></h2>"""

# Training Random Forest Model
# This code trains a Random Forest model to predict runs, and prints out how well the model performed using MAE and R2 metrics
rf_T20_bowl = RandomForestRegressor(n_estimators=100, random_state=42)
rf_T20_bowl.fit(X_train, y_train)

# Predict on test data
y_pred = rf_T20_bowl.predict(X_test)
y_pred_rounded = np.round(y_pred).astype(int)

# Evaluation
mae = mean_absolute_error(y_test, y_pred_rounded)
r2 = r2_score(y_test, y_pred_rounded)

print(f"Random Forest (Wickets Prediction)")
print(f"MAE: {mae:.3f}")
print(f"R²: {r2:.3f}")

# Create a DataFrame to compare actual and predicted values
results_df = pd.DataFrame({
    'Actual Wickets': y_test.values,
    'Predicted Wickets': y_pred_rounded
})

# Show top 20 rows
print(results_df.head(20))

# Feature importances
importances = pd.Series(rf_T20_bowl.feature_importances_, index=X.columns)
importances_sorted = importances.sort_values(ascending=False)[:20]

# Plot top 20 features
plt.figure(figsize=(10, 6))
importances_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (Random Forest)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">Linear Regression</font></b></h2>"""

# Training Linear Regression Model
#This code trains Linear Regression model to predict runs, and prints out how well the model performed using MAE and R2 metrics
lr_T20_bowl = LinearRegression()
lr_T20_bowl.fit(X_train, y_train)

# Predict on test data
y_pred_lr = lr_T20_bowl.predict(X_test)
y_pred_lr_rounded = np.round(y_pred_lr).astype(int)

# Evaluation
mae_lr = mean_absolute_error(y_test, y_pred_lr_rounded)
r2_lr = r2_score(y_test, y_pred_lr_rounded)

print("🎯 Linear Regression (Wickets Prediction)")
print(f"MAE: {mae_lr:.3f}")
print(f"R²: {r2_lr:.3f}")

# Create a DataFrame to compare actual and predicted values
results_lr_df = pd.DataFrame({
    'Actual Wickets': y_test.values,
    'Predicted Wickets': y_pred_lr_rounded
})

# Show top 20 rows
print(results_lr_df.head(20))

# Feature importances (coefficients)
coefs = pd.Series(lr_T20_bowl.coef_, index=X.columns)
coefs_sorted = coefs.sort_values(ascending=False)[:20]

# Plot top 20 coefficients
plt.figure(figsize=(10, 6))
coefs_sorted.plot(kind='barh')
plt.title("Top 20 Coefficients (Linear Regression)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">XGBoost</font></b></h2>"""

# Training XGBoost Model
#This code trains a XGBoost model to predict runs, and prints out how well the model performed using MAE and R2 metrics
xgb_T20_bowl = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_T20_bowl.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb_T20_bowl.predict(X_test)
y_pred_xgb_rounded = np.round(y_pred_xgb).astype(int)

# Evaluation
mae_xgb = mean_absolute_error(y_test, y_pred_xgb_rounded)
r2_xgb = r2_score(y_test, y_pred_xgb_rounded)

print("🎯 XGBoost Regressor (Wickets Prediction)")
print(f"MAE: {mae_xgb:.3f}")
print(f"R²: {r2_xgb:.3f}")

# Create a DataFrame to compare actual and predicted values
results_xgb_df = pd.DataFrame({
    'Actual Wickets': y_test.values,
    'Predicted Wickets': y_pred_xgb_rounded
})

# Show top 20 rows
print(results_xgb_df.head(20))

# Feature importances
importances_xgb = pd.Series(xgb_T20_bowl.feature_importances_, index=X.columns)
importances_xgb_sorted = importances_xgb.sort_values(ascending=False)[:20]

# Plot top 20 features
plt.figure(figsize=(10, 6))
importances_xgb_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (XGBoost)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">LightGBM</font></b></h2>"""

# Training LightGBM Model
#This code trains a LightGBM model to predict runs, and prints out how well the model performed using MAE and R2 metrics
import lightgbm as lgb

X.columns = X.columns.str.replace(' ', '_')

lgb_T20_bowl = lgb.LGBMRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=10,
    num_leaves=31,
    random_state=42
)
lgb_T20_bowl.fit(X_train, y_train)

# Predict on test data
y_pred_lgb = lgb_T20_bowl.predict(X_test)
y_pred_lgb_rounded = np.round(y_pred_lgb).astype(int)

# Evaluation
mae_lgb = mean_absolute_error(y_test, y_pred_lgb_rounded)
r2_lgb = r2_score(y_test, y_pred_lgb_rounded)

print("🎯 LightGBM Regressor (Wickets Prediction)")
print(f"MAE: {mae_lgb:.3f}")
print(f"R²: {r2_lgb:.3f}")

joblib.dump(lgb_T20_bowl, "lgb_reg_t20_bowl.joblib")

# Feature importances
importances_lgb = pd.Series(lgb_T20_bowl.feature_importances_, index=X.columns)
importances_lgb_sorted = importances_lgb.sort_values(ascending=False)[:20]

# Plot top 20 features
plt.figure(figsize=(10, 6))
importances_lgb_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (LightGBM)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">Standardization</font></b></h2>"""

# Fix column names before scaling
X.columns = X.columns.str.replace(' ', '_')

# Standardizing features and preserving column names
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Train-test split
X_train_std, X_test_std, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_std, y_train)
y_pred_rf = np.round(rf.predict(X_test_std)).astype(int)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest (Rounded): MAE: {mae_rf:.3f} R2: {r2_rf:.3f}")

# Linear Regression
lr = LinearRegression()
lr.fit(X_train_std, y_train)
y_pred_lr = np.round(lr.predict(X_test_std)).astype(int)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)
print(f"Linear Regression (Rounded): MAE: {mae_lr:.3f} R2: {r2_lr:.3f}")

# XGBoost
xgb_reg = XGBRegressor(random_state=42, verbosity=0)
xgb_reg.fit(X_train_std, y_train)
y_pred_xgb = np.round(xgb_reg.predict(X_test_std)).astype(int)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost (Rounded): MAE: {mae_xgb:.3f} R2: {r2_xgb:.3f}")

# LightGBM
lgb_reg = LGBMRegressor(random_state=42)
lgb_reg.fit(X_train_std, y_train)
y_pred_lgb = np.round(lgb_reg.predict(X_test_std)).astype(int)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)
print(f"LightGBM (Rounded): MAE: {mae_lgb:.3f} R2: {r2_lgb:.3f}")

"""<h2><b><font color="gold">Normalization</font></b></h2>"""

# Fix column names
X.columns = X.columns.str.replace(' ', '_')

# Normalize features to [0, 1]
scaler = MinMaxScaler()
X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Train-test split
X_train_norm, X_test_norm, y_train, y_test = train_test_split(
    X_normalized, y, test_size=0.2, random_state=42
)

# Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_norm, y_train)
y_pred_rf = np.round(rf.predict(X_test_norm)).astype(int)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest (Rounded): MAE: {mae_rf:.3f} R2: {r2_rf:.3f}")

# Linear Regression
lr = LinearRegression()
lr.fit(X_train_norm, y_train)
y_pred_lr = np.round(lr.predict(X_test_norm)).astype(int)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)
print(f"Linear Regression (Rounded): MAE: {mae_lr:.3f} R2: {r2_lr:.3f}")

# XGBoost
xgb_reg = XGBRegressor(random_state=42, verbosity=0)
xgb_reg.fit(X_train_norm, y_train)
y_pred_xgb = np.round(xgb_reg.predict(X_test_norm)).astype(int)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost (Rounded): MAE: {mae_xgb:.3f} R2: {r2_xgb:.3f}")

# LightGBM
lgb_reg = LGBMRegressor(random_state=42)
lgb_reg.fit(X_train_norm, y_train)
y_pred_lgb = np.round(lgb_reg.predict(X_test_norm)).astype(int)
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)
print(f"LightGBM (Rounded): MAE: {mae_lgb:.3f} R2: {r2_lgb:.3f}")

"""<h2><b><font color="gold">LightGBM Hyperparameter Tuning</font></b></h2>

"""

# Defining the parameter grid
param_dist = {
    'num_leaves': [15, 31, 63, 127],
    'max_depth': [-1, 5, 10, 15],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 300, 500],
    'min_child_samples': [5, 10, 20, 30],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# Initialize model
lgbm = LGBMRegressor(random_state=42)

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=lgbm,
    param_distributions=param_dist,
    n_iter=30,  # reduce if very slow
    scoring='neg_mean_absolute_error',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Fit the random search
random_search.fit(X_train, y_train)

# Predicting and evaluating
best_lgbm = random_search.best_estimator_
y_pred = np.round(best_lgbm.predict(X_test)).astype(int)

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Best Parameters:", random_search.best_params_)
print(f"Tuned LightGBM (Rounded): MAE: {mae:.3f}, R2: {r2:.3f}")

results_simple = pd.DataFrame({
    'Actual': y_test.values.astype(int),
    'Predicted (rounded)': y_pred.astype(int)
})
results_simple['Error'] = results_simple['Predicted (rounded)'] - results_simple['Actual']

print(results_simple.head(20))

# Use your tuned model from RandomizedSearchCV
importances_lgb = pd.Series(best_lgbm.feature_importances_, index=X.columns)
importances_lgb_sorted = importances_lgb.sort_values(ascending=False).head(20)

plt.figure(figsize=(10, 6))
importances_lgb_sorted.plot(kind='barh')
plt.title("Top 20 Feature Importances (Tuned LightGBM)")
plt.gca().invert_yaxis()
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

"""<h2><b><font color="gold">SHAP explanations for tuned LightGBM </font></b></h2>"""

import shap

# Use a small sample of X_test for speed (uses all if smaller than 1000)
X_explain = X_test.sample(n=min(1000, len(X_test)), random_state=42)

# Build explainer and compute SHAP values
explainer = shap.TreeExplainer(best_lgbm)
shap_values = explainer.shap_values(X_explain)

"""***SHAP Beeswarm Plot***"""

# SHAP beeswarm showing impact of each feature on individual predictions
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,
    X_explain,
    max_display=10,
    show=False
)
plt.title("SHAP value (impact on model output)")
plt.tight_layout()
plt.show()

"""***SHAP Bar Plot***

"""

# SHAP bar chart showing average absolute impact of each feature
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,
    X_explain,
    plot_type="bar",
    max_display=10,
    show=False
)
plt.title("mean(|SHAP value|) – average impact on model output")
plt.xlabel("mean(|SHAP value|) (average impact on model output)")
plt.tight_layout()
plt.show()

"""***SHAP Single-Feature Dependence Plot***"""

# SHAP dependence plot showing how one feature’s values affect predictions

# Pick the top feature by mean absolute SHAP value
mean_abs = np.abs(shap_values).mean(axis=0)
top_idx = np.argsort(mean_abs)[-1]
top_feat = X_explain.columns[top_idx]   # e.g., "vs_venue_wickets"

plt.figure(figsize=(10, 6))
shap.dependence_plot(
    top_feat,          # feature to visualize
    shap_values,       # SHAP values from explainer
    X_explain,         # data sample used for explanations
    show=False
)
plt.title(f"SHAP Dependence Plot – {top_feat}")
plt.tight_layout()
plt.show()

